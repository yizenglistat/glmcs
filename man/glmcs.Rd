% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model.R
\name{glmcs}
\alias{glmcs}
\title{Likelihood-based Additive Single-Effect Regression for Generalized Linear Models}
\usage{
glmcs(
  X,
  y,
  family = gaussian("identity"),
  L = 10L,
  coverage = 0.95,
  standardize = TRUE,
  ties = c("efron", "breslow"),
  algorithm = c("greedy", "shuffle", "cyclic"),
  max_iter = 100L,
  step_size = 1,
  min_abs_corr = 0,
  tol = 1,
  seed = NULL
)
}
\arguments{
\item{X}{A numeric matrix with n rows (observations) and p columns (predictors).}

\item{y}{Response variable:
\itemize{
\item For GLMs: a numeric vector of length n
\item For Cox: a matrix with n rows and 2 columns (time, status)
}}

\item{family}{A GLM family object (e.g., \code{gaussian()}, \code{binomial()},
\code{poisson()}, \code{Gamma()}, \code{inverse.gaussian()}) or the
string \code{"cox"} for Cox proportional hazards.}

\item{L}{Integer. Number of single-effect components to include in the model.
Default: 10, or min(10, p) if p < 10.}

\item{coverage}{Numeric in (0,1). Desired posterior mass for confidence sets.
Default: 0.95.}

\item{standardize}{Logical. Whether to standardize predictors before fitting.
Default: TRUE.}

\item{ties}{String. Tie-handling method for Cox regression: "efron" (more accurate)
or "breslow" (faster). Default: "efron".}

\item{algorithm}{String. Coordinate ascent update strategy:
\itemize{
\item "greedy": Select update giving maximum improvement (default)
\item "shuffle": Update effects in random order each iteration
\item "cyclic": Update effects in fixed order
}}

\item{max_iter}{Integer. Maximum number of coordinate ascent iterations.
Default: 100.}

\item{step_size}{Numeric > 0. Multiplicative step-size for updates. Values < 1
provide more conservative updates. Default: 1.0.}

\item{min_abs_corr}{Numeric in [0,1). Minimum absolute correlation threshold
for purity filtering of confidence sets. Default: 0 (no filtering).}

\item{tol}{Numeric > 0. Convergence tolerance on log-likelihood change.
Default: 1.0.}

\item{seed}{Integer or NULL. Random seed for reproducibility of random operations.
Default: NULL (no seed).}
}
\value{
A list of class "glmcs" with components:
\describe{
\item{\code{elapsed}}{Numeric. Total fitting time in seconds.}
\item{\code{fit}}{List. Raw output from \code{\link{get_laser_fit}}.}
\item{\code{pmp}}{Numeric vector. Posterior marginal probabilities of inclusion.}
\item{\code{theta}}{Numeric vector. Estimated effect vector (sum over significant components).}
\item{\code{cs}}{List. Confidence sets (list of integer vectors) from \code{\link{get_cs}}.}
\item{\code{coverage}}{Numeric. Requested coverage level.}
\item{\code{keep}}{Logical vector. Indicators for significant effects from \code{\link{get_included}}.}
\item{\code{X}, \code{y}, \code{family}, \code{ties}, \code{algorithm},
\code{step_size}, \code{seed}}{The original input parameters.}
\item{\code{convergence}}{Logical. Whether the algorithm converged within max_iter.}
}
}
\description{
Fits the LASER (Likelihood-based Additive Single-Effect Regression) model via
block coordinate ascent to perform variable selection under multicollinearity
and build confidence sets at a specified coverage level. Supports both generalized
linear models (GLMs) and Cox proportional hazards regression.
}
\details{
The LASER approach decomposes the coefficient vector into a sum of L sparse
effects, each fitted using a Bayesian model averaging approach. This implementation:
\enumerate{
\item Fits the LASER model using block coordinate ascent
\item Identifies statistically significant effects
\item Computes confidence sets with desired posterior coverage
\item Filters confidence sets based on correlation purity (if requested)
\item Calculates posterior marginal probabilities of inclusion
}

The algorithm is particularly effective for high-dimensional, multicollinear
data where traditional methods struggle to identify relevant variables.
}
\examples{
# Generate synthetic data with 2 true variables
set.seed(42)
n <- 100; p <- 20
X <- matrix(rnorm(n * p), n, p)
X[,1:2] <- X[,1:2] \%*\% matrix(c(1, 0.9, 0.9, 1), 2, 2) # Create correlation
beta <- c(3, 0, rep(0, p-2)) # Only first variable is truly active
y <- X \%*\% beta + rnorm(n)

# Fit with LASER model
res <- glmcs(
  X           = X,
  y           = y,
  family      = gaussian(),
  L           = 5L,
  coverage    = 0.9,
  standardize = TRUE,
  algorithm   = "greedy"
)

# Examine results
print(res$theta)       # Estimated coefficients
print(res$cs)          # Confidence sets
print(res$pmp)         # Posterior marginal probabilities

# Logistic regression example
y_bin <- rbinom(n, 1, plogis(X \%*\% beta))
res_bin <- glmcs(X, y_bin, family = binomial())

}
\seealso{
\code{\link{get_laser_fit}} for the low-level C++ fit routine,
\code{\link{get_included}} to select significant effects,
\code{\link{get_cs}} to build confidence sets
}
