% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/internal.R
\name{kl_divergence}
\alias{kl_divergence}
\title{Kullback-Leibler Divergence from Uniform Distribution}
\arguments{
\item{p}{Numeric vector. Probability distribution vector that should sum to 1.}

\item{log_base}{Numeric. Base of the logarithm used in calculation (default: e = 2.718...).
Common alternatives include 2 (bits) or 10.}
}
\value{
Numeric. KL divergence value (always non-negative). A value of 0 indicates
that \code{p} is exactly uniform, while larger values indicate greater
divergence from uniformity.
}
\description{
Calculates the Kullback-Leibler (KL) divergence between a given probability
distribution \code{p} and a uniform distribution of equal length. KL divergence
measures how much one probability distribution diverges from another
expected distribution.

In the LASER modeling framework, this function helps quantify how much a
posterior distribution deviates from uniformity, which is useful for
identifying significant effects.
}
\details{
The KL divergence is calculated as:
\deqn{D_{KL}(P||U) = \sum_{i=1}^{n} P(i) \log\left(\frac{P(i)}{U(i)}\right)}
where \eqn{U(i) = 1/n} is the uniform distribution.

For numerical stability, this implementation:
\enumerate{
\item Only processes positive probability elements (zeros contribute nothing)
\item Allows for different logarithm bases
}

The KL divergence is always non-negative and equals zero if and only if
the distributions are identical (i.e., \code{p} is perfectly uniform).
}
\examples{
# Uniform distribution (KL = 0)
p1 <- rep(0.25, 4)
kl_divergence(p1)  # Should return 0

# Somewhat skewed distribution
p2 <- c(0.1, 0.2, 0.3, 0.4)
kl_divergence(p2)  # Positive value

# Very concentrated distribution
p3 <- c(0.01, 0.01, 0.97, 0.01)
kl_divergence(p3)  # Large positive value

# Using binary logarithm (base 2)
kl_divergence(p3, log_base = 2)

}
\references{
Kullback, S., & Leibler, R. A. (1951). On information and sufficiency.
The Annals of Mathematical Statistics, 22(1), 79-86.
}
\seealso{
\code{\link{get_included}} which uses KL divergence to identify significant effects
}
