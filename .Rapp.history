library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
compute_pip_bic <- function(X, Y) {#
  p <- ncol(X)#
  n <- nrow(X)#
  BICs <- numeric(p)#
#
  for (j in 1:p) {#
    fit <- lm(Y ~ X[, j])#
    BICs[j] <- BIC(fit)#
  }#
#
  # Compute Bayes Factors#
  min_BIC <- min(BICs)#
  BF <- exp(-0.5 * (BICs - min_BIC))#
  PIP <- BF / sum(BF)#
  return(PIP)#
}
compute_pip_bic(X,Y)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, (weight_alasso>1)*1)
(weight_alasso>1)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, (weight_alasso>0)*1)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 1), c(1, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 5000#
p <- 4#
Sigma <- rbind(c(1, 1), c(1, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 100#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 3#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 100#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 0, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
lasso_cd <- function(X, Y, lambda, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}
set.seed(42)#
n <- 100#
p <- 5#
X <- matrix(rnorm(n * p), n, p)#
beta_true <- c(2, -1.5, 0, 0, 0)#
Y <- X %*% beta_true + rnorm(n)#
#
lambda <- 1.0#
beta_hat <- lasso_cd(X, Y, lambda)#
print(round(beta_hat, 3))
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}
lasso(X,Y)
lasso(X,Y,10)
lasso(X,Y,7)
lasso(X,Y,5)
lasso(X,Y,3)
lasso(X,Y,10)
get_bic
pip
gift
output_file <- ifelse(num_samples < 1000, glue("output/2sls_n{num_samples}.txt"), glue("output/2sls_n{num_samples/1000}K.txt"))#
#
sink(output_file)#
#
print(num_samples)#
#
susie 		<- round(apply(pip_susie,2,mean), 2)#
frsusie 	<- round(apply(pip_frsusie,2,mean), 2)#
fsusie 		<- round(apply(pip_asusie,2,mean), 2)#
ffrsusie 	<- round(apply(pip_afrsusie,2,mean), 2)#
cfrsusie 	<- round(apply(pip_cfrsusie,2,mean), 2)#
pip <- data.frame(gift=NA, susie=susie, frsusie=frsusie, fsusie=fsusie, ffrsusie=ffrsusie, cfrsuise=cfrsuise)#
#
gift 		<- round(apply(beta_gift,2,mean), 4)#
susie 		<- round(apply(beta_susie,2,mean), 4)#
frsusie 	<- round(apply(beta_frsusie,2,mean), 4)#
fsusie 		<- round(apply(beta_asusie,2,mean), 4)#
ffrsusie 	<- round(apply(beta_afrsusie,2,mean), 4)#
cfrsusie 	<- round(apply(beta_cfrsusie,2,mean), 4)#
theta <- data.frame(cbind(gift, susie, frsusie, fsusie, ffrsusie, cfrsusie))#
#
gift 		<- round(apply(beta_gift,2,sd), 4)#
susie 		<- round(apply(beta_susie,2,sd), 4)#
frsusie 	<- round(apply(beta_frsusie,2,sd), 4)#
fsusie 		<- round(apply(beta_asusie,2,sd), 4)#
ffrsusie 	<- round(apply(beta_afrsusie,2,sd), 4)#
cfrsusie 	<- round(apply(beta_cfrsusie,2,sd), 4)#
ssd <- data.frame(cbind(gift, susie, frsusie, fsusie, ffrsusie, cfrsusie))#
#
# for(i in 1:1000) if(length(cs_susie[[i]])==0) cs_susie[[i]] <- NA#
# for(i in 1:1000) if(length(cs_frsusie[[i]])==0) cs_frsusie[[i]] <- NA#
#
latex_lines <- get_2sls_table(pip, theta, ssd)#
cat(paste(latex_lines, collapse = "\n"))#
#
cat("\n")#
get_vs5_counts(vs_gift)#
get_cs_counts(cs_susie)#
get_cs_counts(cs_frsusie)#
get_cs_counts(cs_asusie)#
get_cs_counts(cs_afrsusie)#
get_cs_counts(cs_cfrsusie)#
#
sink()
cfrsusie
get_est <- function(x, y) {#
  b_hat <- sum(x*y) / sum(x^2)#
  sigma2_hat <- sum((y - b_hat*x)^2)/(length(y)-1)#
  se_hat <- sqrt(sigma2_hat / sum(x^2))#
  return(list(b_hat=b_hat, sigma2_hat=sigma2_hat, se_hat=se_hat))#
}#
#
get_bf <- function(x, y, sigma0=1) {#
  res <- get_est(x,y)#
  b_hat <- res$b_hat#
  sigma2_hat <- res$sigma2_hat#
  s2 <- sigma2_hat / sum(x^2)#
  z <- b_hat / sqrt(s2)#
  BF <- sqrt(s2/(sigma0^2 + s2)) * exp((z^2/2) * (sigma0^2/(sigma0^2+s2)))#
  return(BF)#
}#
#
get_bic <- function(x, y) {#
  res <- get_est(x,y)#
  b_hat <- res$b_hat#
  n <- length(y)#
  BIC <- n*log(sum((y-b_hat*x)^2)/(sum(y^2))) + log(n)#
  return(BIC)#
}#
#
get_pval <- function(x, y) {#
  res <- get_est(x, y)#
  z_score <- res$b_hat / res$se_hat#
  p_val <- 2 * (1 - pt(abs(z_score), length(y)-1))  # two-tailed test#
  return(p_val)#
}#
get_power <- function(x, y, num_sim = 1000, alpha = 0.05) {#
  res <- get_est(x, y)#
  beta_hat <- res$b_hat#
  se_hat <- res$se_hat#
  n <- length(y)#
  reject_count <- 0#
  for (i in 1:num_sim) {#
    # Simulate new Y under H1: Y = X*beta_hat + noise#
    y_sim <- beta_hat * x + rnorm(n, mean = 0, sd = se_hat)#
    # Compute p-value for simulated data#
    p_val <- get_pval(x, y_sim)#
    # Count rejection cases#
    if (p_val < alpha) {#
      reject_count <- reject_count + 1#
    }#
  }#
  # Compute empirical power#
  power_estimate <- reject_count / num_sim#
  return(power_estimate)#
}#
#
get_pip <- function(X, y, beta_mat, pip_mat) {#
  L <- ncol(pip_mat)#
  bic <- numeric(L)#
  n <- length(y)#
  residuals <- y - rowSums(X%*%beta_mat)#
  for(l in 1:L) {#
    r <- residuals + X%*%beta_mat[,l]#
    bic[l] <- n*log(sum(residuals^2)/sum(r^2)) + log(n)#
  }#
#
  res <- remove_null_effect(X, y, beta_mat)#
  include_index <- (bic < 0) & (!res$remove)#
  pip <- 1 - apply(1-pip_mat[,include_index,drop=FALSE], 1, prod)#
  return(list(pip=pip, pvals=res$p_values, include_index=include_index))#
}#
#
remove_null_effect <- function(X, y, beta_mat, p_threshold = 0.05) {#
  n <- length(y)#
  L <- ncol(beta_mat)#
#
  remove_effects <- rep(FALSE, L)#
  delta_sigma2_vec <- rep(NA, L)#
  p_values <- rep(NA, L)#
#
  residuals_full <- y - X %*% rowSums(beta_mat)#
#
  for (l in 1:L) {#
    residuals_partial <- residuals_full + X %*% beta_mat[, l]#
    sigma2_full <- mean(residuals_full^2)#
    sigma2_partial <- mean(residuals_partial^2)#
    # Compute variance reduction#
    delta_sigma2 <- sigma2_partial - sigma2_full#
    # Likelihood Ratio Test (LRT)#
    LRT_stat <- n * (log(sigma2_partial) - log(sigma2_full))#
    p_value <- pchisq(LRT_stat, df = 1, lower.tail = FALSE)#
    # Remove component if variance reduction is negligible or LRT suggests no improvement#
    remove_effects[l] <- (p_value > p_threshold)#
    # Store results#
    delta_sigma2_vec[l] <- delta_sigma2#
    p_values[l] <- p_value#
  }#
  return(list(#
    remove = remove_effects,#
    delta_sigma2 = round(delta_sigma2_vec, 4),#
    p_values = round(p_values, 4)#
   ))#
}#
#
ser <- function(X, y) {#
  p <- ncol(X)#
  n <- length(y)#
#
  bf <- numeric(p)#
  bic <- numeric(p)#
  bf_hat <- numeric(p)#
  beta_hat <- numeric(p)#
  pval_hat <- numeric(p)#
#
  for(j in 1:p) {#
    bf[j] <- get_bf(X[,j], y)#
    bic[j] <- get_bic(X[,j], y)#
#
    beta_hat[j] <- get_est(X[,j], y)$b_hat#
    pval_hat[j] <- get_pval(X[,j], y)#
  }#
#
  # Numerically stable PIP approximation#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  alpha_hat <- bf_hat / sum(bf_hat)#
#
  return(list(alpha_hat=alpha_hat, beta_hat=beta_hat, pval_hat=pval_hat))#
}#
#
get_cs_counts <- function(results) {#
  for(i in 1:length(results)) if(length(results[[i]])==0) results[[i]] <- NA#
  confidence_sets <- sapply(results, function(cs) {#
    # Sort each individual confidence set internally#
    formatted_sets <- sapply(cs, function(set) {#
      if (length(set) == 0) {#
        return("()")  # Handle empty sets explicitly#
      } else {#
        return(paste0("(", paste(sort(set), collapse = ","), ")"))#
      }#
    })#
    # Sort the entire set of confidence sets to ensure consistency#
    paste(sort(formatted_sets), collapse = ", ")#
  })#
  # Count unique confidence sets#
  unique_counts <- table(confidence_sets)#
  # Convert to data frame#
  unique_df <- as.data.frame(unique_counts)#
  colnames(unique_df) <- c("set", "count")#
#
  # Sort by count in descending order for readability#
  unique_df <- unique_df[order(-unique_df$count), ]#
  return(unique_df)#
}#
#
get_vs_counts <- function(beta_matrix) {#
  all_zero <- sum(beta_matrix[,1] == 0 & beta_matrix[,2] == 0)#
  x1_nonzero <- sum(beta_matrix[,1] != 0 & beta_matrix[,2] == 0)#
  x2_nonzero <- sum(beta_matrix[,1] == 0 & beta_matrix[,2] != 0)#
  both_nonzero <- sum(beta_matrix[,1] != 0 & beta_matrix[,2] != 0)#
  return(data.frame(#
    all_zero = all_zero,#
    x1_only = x1_nonzero,#
    x2_only = x2_nonzero,#
    both_nonzero = both_nonzero#
  ))#
}#
#
get_vs2_counts <- function(beta_matrix) {#
  A <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  B <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  C <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  D <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  out <- c(A,B,C,D)#
  return(out)#
}#
#
get_vs3_counts <- function(beta_matrix) {#
  R23 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]==0)))#
  R134 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R234 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R123 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]==0)))#
  R124 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)))#
  R1234 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R2345 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]!=0)))#
  R1235 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]!=0)))#
  out <- c(R23, R134, R234, R123, R124, R1234, R2345, R1235)#
  return(out)#
}#
#
get_vs4_counts <- function(pval_matrix, alpha=0.05) {#
  R134 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R1234 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R34 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R234 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R124 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R14 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R4 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
#
  out <- c(R134, R1234, R34, R234, R124, R14, R4)#
  return(out)#
}#
#
get_vs5_counts <- function(pval_matrix, alpha=0.05) {#
  R14 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R13 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]>alpha)&(x[5]>alpha)))#
  R23 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]>alpha)&(x[5]>alpha)))#
  R24 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
#
  R124 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R134 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R234 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R1234 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  out <- c(R14, R124, R134, R1234)#
  return(out)#
}#
reindex_features <- function(X, G) {#
  unique_features <- sort(unique(as.vector(G)))#
  X_subset <- X[, unique_features, drop = FALSE]#
  new_indices <- seq_along(unique_features)  # New indices: 1 to ncol(X_subset)#
  old_to_new <- setNames(new_indices, unique_features)#
  G_subset <- matrix(old_to_new[as.character(as.vector(G))], nrow = 2, byrow = FALSE)#
  return(list(X = X_subset, G = G_subset))#
}#
#
likeli_ratio <- function(y, X, beta, sigma = 1) {#
  (sum((y - X %*% beta)^2) - sum(y^2)) / sigma^2#
}#
#
loss <- function(y, X, beta) {#
  mean((y - X %*% beta)^2)#
}#
#
compute_recovery_rate <- function(beta_hat, beta_true) {#
  true_nonzero_indices <- which(beta_true != 0)#
  hat_nonzero_indices <- which(beta_hat != 0)#
  correct_indices <- intersect(true_nonzero_indices, hat_nonzero_indices)#
  oracle_recovery_rate <- length(correct_indices) / length(true_nonzero_indices) * 100#
  return(oracle_recovery_rate)#
}#
#
mse <- function(x, x_hat) {#
  mean((x-x_hat)^2)#
}#
#
n_in_CS_x = function (x, coverage = 0.95) #
  sum(cumsum(sort(x,decreasing = TRUE)) < coverage) + 1#
#
in_CS_x = function (x, coverage = 0.95) {#
  n = n_in_CS_x(x,coverage)#
  o = order(x,decreasing = TRUE)#
  result = rep(0,length(x))#
  result[o[1:n]] = 1#
  return(result)#
}#
#
in_CS = function (alpha, coverage = 0.95) {#
  #return(filter_status(t(apply(t(alpha), 1, function(x) in_CS_x(x,coverage)))))#
  return(t(apply(t(alpha), 1, function(x) in_CS_x(x,coverage))))#
}#
#
filter_status <- function(mat) {#
  keep <- rep(TRUE, nrow(mat))  # Vector to track rows to keep#
  covered <- rep(0, ncol(mat))  # Track covered columns#
  for (i in 1:nrow(mat)) {#
    ones <- which(mat[i, ] == 1)  # Find columns with 1s in the current row#
    if (any(covered[ones] == 1)) {  # If all 1s are already covered, mark row for removal#
      keep[i] <- FALSE#
    } else {#
      covered[ones] <- 1  # Mark columns as covered#
    }#
  }#
  return(mat[keep, , drop = FALSE])#
}#
#
get_cs <- function(pip_mat, include_index, #
  X=NULL, Xcorr=NULL, check_symmetric=TRUE,#
  min_abs_corr=0.5, n_purity=100, squared=FALSE, #
  coverage = 0.95, use_rfast) {#
#
  if(sum(include_index)==0) {#
    return(list(sets = NULL, coverage = NULL))#
  }#
#
  pip_mat <- pip_mat[, include_index, drop=FALSE]#
#
  status <- in_CS(pip_mat, coverage)#
  cs <- lapply(1:nrow(status), function(i) sort(which(status[i, ] != 0)))#
  # remove duplication#
  unique_cs_strings <- unique(sapply(cs, function(set) paste(set, collapse = ",")))#
  unique_cs <- lapply(unique_cs_strings, function(str) as.integer(unlist(strsplit(str, ","))))#
  claimed_coverage <- sapply(unique_cs, function(set) ifelse(sum(pip_mat[set, ])>1, 1, sum(pip_mat[set, ])))#
  # purity#
  if (!is.null(X) && !is.null(Xcorr))#
    stop("Only one of X or Xcorr should be specified")#
  if (check_symmetric) {#
    if (!is.null(Xcorr) && !is_symmetric_matrix(Xcorr)) {#
      warning_message("Xcorr is not symmetric; forcing Xcorr to be symmetric",#
                  "by replacing Xcorr with (Xcorr + t(Xcorr))/2")#
      Xcorr = Xcorr + t(Xcorr)#
      Xcorr = Xcorr/2#
    }#
  }#
#
  if (is.null(Xcorr) && is.null(X)) {#
    return(list(sets = unique_cs, coverage = claimed_coverage))#
  } else {#
#
    purity = NULL#
    for(idx in 1:length(unique_cs)) {#
      purity <- rbind(purity, matrix(get_purity(unique_cs[[idx]], X, Xcorr, squared, n_purity, use_rfast),1,3))#
    }#
#
    purity = as.data.frame(purity)#
    rownames(purity) <- NULL#
    if (squared) {#
      colnames(purity) = c("min.sq.corr","mean.sq.corr","median.sq.corr")#
    }#
    else {#
      colnames(purity) = c("min.abs.corr","mean.abs.corr","median.abs.corr")#
    }#
#
    threshold = ifelse(squared, min_abs_corr^2, min_abs_corr)#
#
    is_pure = which(purity[,1] >= threshold)#
#
    unique_cs <- unique_cs[is_pure]#
    claimed_coverage <- claimed_coverage[is_pure]#
    purity <- purity[is_pure,]#
    ordering = order(purity[,1],decreasing = TRUE)#
#
    return(list(sets = unique_cs[ordering], #
                coverage = claimed_coverage[ordering],#
                purity   = purity[ordering,]#
                ))#
  }#
}#
#
muffled_corr = function (x)#
  withCallingHandlers(cor(x),#
                      warning = function(w) {#
                        if (grepl("the standard deviation is zero",w$message))#
                          invokeRestart("muffleWarning")#
                      })#
#
get_purity = function (pos, X, Xcorr=NULL, squared = FALSE, n = 100,#
                       use_rfast) {#
  if (missing(use_rfast))#
    use_rfast = requireNamespace("Rfast",quietly = TRUE)#
  if (use_rfast) {#
    # get_upper_tri = Rfast::upper_tri#
    get_upper_tri = function (R) R[upper.tri(R)]#
    get_median    = Rfast::med#
  } else {#
    get_upper_tri = function (R) R[upper.tri(R)]#
    get_median    = stats::median#
  }#
  if (length(pos) == 1)#
    return(c(1,1,1))#
  else {#
#
    # Subsample the columns if necessary.#
    if (length(pos) > n)#
      pos = sample(pos,n)#
#
    if (is.null(Xcorr)) {#
      X_sub = X[,pos]#
      X_sub = as.matrix(X_sub)#
      value = abs(get_upper_tri(muffled_corr(X_sub)))#
    } else#
      value = abs(get_upper_tri(Xcorr[pos,pos]))#
    if (squared)#
      value = value^2#
    return(c(min(value),#
             sum(value)/length(value),#
             get_median(value)))#
  }#
}#
#
fill_vector <- function(vec, p) {#
  setNames(replace(numeric(p), match(names(vec), paste0("X", 1:p)), vec), paste0("X", 1:p))#
}#
#
log_likelihood <- function(X, y, beta_hat, sigma_hat) {#
  n <- length(y)#
  residuals <- y - X %*% beta_hat#
  log_lik <- - (n / 2) * log(2 * pi * sigma_hat^2) - (1 / (2 * sigma_hat^2)) * sum(residuals^2)#
  return(log_lik)#
}#
rmse <- function(x, x_hat) sqrt(mean((x-x_hat)^2))#
#
generate_gamma <- function(num_instruments_block, num_features) {#
  return(matrix(runif(num_instruments_block*num_features, 0.3, 0.5), num_instruments_block, num_features))#
}#
#
generate_alpha <- function(num_instruments, indexes=c(11:15)) {#
  if(length(indexes)>0){#
    alpha <- rnorm(num_instruments, 0.5, 0.5)#
    alpha[-indexes] <- 0#
  }else{#
    alpha <- rep(0, num_instruments)#
  }#
  return(alpha)#
}#
# =====================================================================================#
get_gamma_mvmr <- function(m, p, weight=0.5, K=0){#
  gamma_shared <- runif(m, 0.0, 0.2)#
  gamma <- matrix(NA, m, p)#
  for (j in 1:p) {#
    gamma_specific <- runif(m, 0.2, 0.4)#
    gamma[, j] <- weight * gamma_shared + (1 - weight) * gamma_specific#
  }#
#
  if(K>0) {#
    invalid_set <- sample(1:m, K)#
    gamma[invalid_set, ] <- rep(0, p)#
  }#
#
  return(gamma)#
}#
#
get_pleiotropy <- function(num_invalid, num_instruments, mu_alpha=0, sd_alpha=0.2, min_varphi=0, max_varphi=0.1) {#
  invalid_set <- sample(1:num_instruments, num_invalid)#
  varphi <- rep(0, num_instruments)#
  varphi[invalid_set] <- runif(num_invalid, min_varphi, max_varphi)#
  alpha <- rep(0, num_instruments)#
  alpha[invalid_set] <- rnorm(num_invalid, mu_alpha, sd_alpha)#
  return(list(varphi=varphi, alpha=alpha, invalid_set=invalid_set))#
}#
#
get_gwas <- function(X, Y, Z) {#
  n <- nrow(Z)#
  m <- ncol(Z)#
  p <- ncol(X)#
#
  Z_var <- colSums(Z^2)#
#
  beta_hat_x <- t(Z) %*% X / matrix(Z_var, nrow = m, ncol = p)#
  sigma_hat_x <- matrix(NA, m, p)#
#
  for (j in 1:p) {#
    res_x <- X[, j] - Z %*% beta_hat_x[, j]#
    sigma_hat_x[, j] <- sqrt(colSums(matrix(res_x^2, n, m)) / (n - 2)) / sqrt(Z_var)#
  }#
#
  beta_hat_y <- as.vector((t(Z) %*% Y) / Z_var)#
  res_y <- Y - Z %*% beta_hat_y#
  sigma_hat_y <- sqrt(colSums(matrix(res_y^2, n, m)) / (n - 2)) / sqrt(Z_var)#
#
  return(list(#
    beta_hat_x = beta_hat_x,#
    beta_hat_y = beta_hat_y,#
    sigma_hat_x = sigma_hat_x,#
    sigma_hat_y = sigma_hat_y#
  ))#
}#
#
# MVMR#
get_bic_K <- function(logL, N, K) return(-2*logL + log(N)*K)#
#
get_invcov <- function(sigma_hat_x, sigma_hat_y, rho_hat_xy){#
  m <- nrow(sigma_hat_x)#
  p <- ncol(sigma_hat_x)#
  Sigma_hat <- array(NA, c(p+1, p+1, m))#
  Sigma_inv_hat <- array(NA, c(p+1, p+1, m))#
  for(k in 1:m){#
    Sig <- rho_hat_xy*crossprod(t(c(sigma_hat_x[k,],sigma_hat_y[k])))#
    Sigma_hat[,,k] <- Sig#
    Sigma_inv_hat[,,k] <- solve(Sig)#
  }#
  return(Sigma_inv_hat)#
}#
#
get_beta_tilde_x <- function(theta_tilde, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  beta_tilde_x <- numeric(m)#
  for(k in 1:m) {#
    numerator <- beta_hat_x[k] / sigma_hat_x[k]^2 + theta_tilde*beta_hat_y[k] / sigma_hat_y[k]^2#
    denominator <- 1/sigma_hat_x[k]^2 + theta_tilde^2 / sigma_hat_y[k]^2#
    beta_tilde_x[k] <- numerator / denominator#
  }#
  return(beta_tilde_x)#
}#
#
get_theta_tilde <- function(beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  numerator <- sum(beta_tilde_x * beta_hat_y / sigma_hat_y^2)#
  denominator <- sum(beta_tilde_x^2 / sigma_hat_y^2)#
  theta_tilde <- numerator / denominator#
  return(theta_tilde)#
}#
#
get_uvmr <- function(beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y, max_iter=100) {#
  m <- length(sigma_hat_y)#
  theta_tilde <- 0#
  beta_tilde <- numeric(m)#
  for(iter in 1:max_iter) {#
    beta_tilde_x <- get_beta_tilde_x(theta_tilde, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y)#
    theta_tilde <- get_theta_tilde(beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) #
  }#
  return(list(theta_tilde=theta_tilde, beta_tilde_x=beta_tilde_x))#
}#
#
get_delta_bic <- function(theta_tilde, beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  delta_bic <- sum(((beta_hat_y - theta_tilde * beta_tilde_x)^2 - beta_hat_y^2) / sigma_hat_y^2 + log(m))#
  return(delta_bic)#
}#
#
get_semr <- function(beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y, max_iter=100) {#
  m <- nrow(beta_hat_x)#
  p <- ncol(beta_hat_x)#
  bic <- numeric(p)#
  theta_tilde <- numeric(p)#
  beta_tilde_x <- matrix(NA, m, p)#
  for(j in 1:p) { #
    res_uvmr <- get_uvmr(beta_hat_x[,j], beta_hat_y, sigma_hat_x[,j], sigma_hat_y, max_iter)#
    theta_tilde[j] <- res_uvmr$theta_tilde#
    beta_tilde_x[,j] <- res_uvmr$beta_tilde_x#
    bic[j] <- get_delta_bic(theta_tilde[j], beta_tilde_x[,j], beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y)#
  }#
#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  omega_tilde <- bf_hat / sum(bf_hat)#
  return(list(omega_tilde=omega_tilde, theta_tilde=theta_tilde, beta_tilde_x=beta_tilde_x))#
}#
#
get_ser <- function(y) {#
  m <- length(y)#
  p <- m#
  X <- matrix(0, m, p)#
  diag(X) <- 1#
  bf <- numeric(p)#
  bic <- numeric(p)#
  bf_hat <- numeric(p)#
  beta_hat <- numeric(p)#
  pval_hat <- numeric(p)#
#
  for(j in 1:p) {#
    bf[j] <- get_bf(X[,j], y)#
    bic[j] <- get_bic(X[,j], y)#
#
    beta_hat[j] <- get_est(X[,j], y)$b_hat#
    pval_hat[j] <- get_pval(X[,j], y)#
  }#
#
  # Numerically stable PIP approximation#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  alpha_hat <- bf_hat / sum(bf_hat)#
#
  return(list(alpha_hat=alpha_hat, beta_hat=beta_hat, pval_hat=pval_hat))#
}#
#
get_Sigma_hat <- function(X, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  p <- ncol(X)#
  Sigma_hat <- array(NA, c(p+1, p+1, m))#
  for(k in 1:m) {#
    Sigma_hat_xk <- matrix(NA, p, p)#
    rho_hat_x <- cor(X)#
    for(i in 1:p){#
      for(j in 1:p){#
        Sigma_hat_xk[i,j] <- rho_hat_x[i,j] * sigma_hat_x[k,i] * sigma_hat_x[k,j]#
      }#
    }#
    Sigma_hat_xyk <- matrix(0, p, 1)#
    Sigma_hat_yxk <- t(Sigma_hat_xyk)#
    Sigma_hat[,,k] <- rbind(cbind(Sigma_hat_xk, Sigma_hat_xyk), #
      c(Sigma_hat_yxk, sigma_hat_y[k]^2))#
  }#
  return(Sigma_hat)#
}#
#
get_log_likelihood <- function(theta_hat, beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, invSigma_hat, nugget=0) {#
  m <- length(beta_hat_y)#
  p <- ncol(beta_tilde_x)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
  out <- 0#
  for(k in 1:m) {#
    beta_hat_k <- c(beta_hat_x[k,], beta_hat_y[k])#
    beta_tilde_k <- c(beta_tilde_x[k,], sum(theta_hat * beta_tilde_x[k,])+r_tilde[k])#
    invSigma_hat_k <- invSigma_hat[,,k]#
    out <- out + t(beta_hat_k - beta_tilde_k) %*% invSigma_hat_k %*% (beta_hat_k - beta_tilde_k)#
  }#
  return(-0.5 * out)#
}#
#
remove_null_mvmr <- function(theta_tilde, beta_tilde_x, r_tilde,#
  beta_hat_x, beta_hat_y, Sigma_hat, nugget=0, threshold = 0.05) {#
  m <- length(beta_hat_y)#
  L <- ncol(theta_tilde)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
  log_lik_full <- get_log_likelihood(rowSums(theta_tilde), beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget)#
  remove_effects <- numeric(L)#
  p_values <- numeric(L)#
  for (l in 1:L) {#
  log_lik_null <- get_log_likelihood(rowSums(theta_tilde[,-l]), beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget)#
    LRT_stat <- -2 * (log_lik_full - log_lik_null) #
    p_value <- pchisq(LRT_stat, df = p, lower.tail = FALSE)#
    # Remove component if variance reduction is negligible or LRT suggests no improvement#
    remove_effects[l] <- (p_value > threshold)#
    p_values[l] <- p_value#
  }#
  return(list(#
    remove = remove_effects,#
    p_values = round(p_values, 4)#
   ))#
}#
kl_divergence <- function(p, base = exp(1)) {#
  q <- rep(1/length(p), length(p))#
  nonzero <- p > 0#
  distance <- sum(p[nonzero] * log(p[nonzero] / q[nonzero], base = base))#
  return(distance)#
}#
#
get_loglik_k <- function(theta_hat, beta_tilde_xk, r_tilde_k, #
  beta_hat_xk, beta_hat_yk, Sigma_inv_hat_k) {#
  beta_hat_k <- c(beta_hat_xk, beta_hat_yk)#
  beta_tilde_k <- c(beta_tilde_xk, sum(beta_tilde_xk * theta_hat) + r_tilde_k)#
  ell_k <- -0.5 * t(beta_hat_k - beta_tilde_k) %*% Sigma_inv_hat_k %*% (beta_hat_k - beta_tilde_k)#
  return(ell_k)#
}#
get_pip_mvmr <- function(omega_tilde, theta_tilde, beta_tilde_x, r_tilde,#
  beta_hat_x, beta_hat_y, Sigma_hat, nugget=0, threshold=0.1) {#
  m <- length(beta_hat_y)#
  L <- ncol(theta_tilde)#
  p <- ncol(beta_tilde_x)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
#
  # res <- remove_null_mvmr(theta_tilde, beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget, threshold)#
  #include_index <- (bic < 0) & (!res$remove)#
  #include_index <- !res$remove#
  dists <- apply(omega_tilde, 2, kl_divergence)#
  include_index <- (dists > threshold)#
  if(all(!include_index)) include_index[which.max(dists)] <- TRUE#
#
  pip <- 1 - apply(1-omega_tilde[,include_index,drop=FALSE], 1, prod)#
  return(list(pip=pip, dists=dists, include_index=include_index))#
}#
#
get_invalid_rates <- function(estimated, truth) {#
  estimated <- sort(unique(estimated))#
  truth <- sort(unique(truth))#
  tp <- length(intersect(estimated, truth))#
  fp <- length(setdiff(estimated, truth))#
  fn <- length(setdiff(truth, estimated))#
  precision <- if ((tp + fp) == 0) NA else tp / (tp + fp)#
  recall <- if ((tp + fn) == 0) NA else tp / (tp + fn)#
  f1 <- if (is.na(precision) || is.na(recall) || (precision + recall) == 0) NA else#
    2 * precision * recall / (precision + recall)#
  fpr <- if (length(estimated) == 0) NA else fp / length(estimated)#
  fnr <- if (length(truth) == 0) NA else fn / length(truth)#
  return(c(fpr, fnr))#
}#
#
get_W <- function(Sigma_inv_hat) {#
  dims <- dim(Sigma_inv_hat)#
  m <- dims[3]#
  p <- dims[1]#
  W_mat <- matrix(NA, nrow = m, ncol = p)#
  W_vec <- numeric(m)#
#
  for (k in 1:m) {#
    W_mat[k, ] <- Sigma_inv_hat[p,,k]         # last row#
    W_vec[k] <- Sigma_inv_hat[p,p,k]          # bottom-right element#
  }#
#
  return(list(mat = W_mat, vec = W_vec))#
}#
#
get_table <- function(theta, ssd, pip, rate) {#
  stopifnot(nrow(theta) == 5, nrow(ssd) == 5, nrow(pip) == 5)#
  beta_labels <- c("$\\beta_1=0.5$", "$\\beta_2=0$", "$\\beta_3=0$", "$\\beta_4=0.5$", "$\\beta_5=0$")#
  format_val_sd <- function(val, sd) {#
    paste0(formatC(val, digits = 4, format = "f", width = 7), #
           " (", formatC(sd, digits = 2, format = "f", width = 4), ")")#
  }#
  format_pip <- function(x) {#
    if (is.na(x)) return("-")#
    formatC(x, digits = 2, format = "f", width = 6)#
  }#
#
  format_cell <- function(x) {#
    if (is.na(x)) return("-")#
    formatC(x, digits = 4, format = "f", width = 7)#
  }#
#
  lines <- character()#
#
  for (i in 1:5) {#
    line1 <- glue::glue(#
      "& \\multirow{{2}}{{*}}{{{beta_labels[i]}}} ",#
      "& Mean (SSD) ",#
      "& {format_val_sd(theta$mvmrbic[i], ssd$mvmrbic[i])} ",#
      "& {format_val_sd(theta$frsusie[i], ssd$frsusie[i])} ",#
      "& {format_val_sd(theta$ffrsusie[i], ssd$ffrsusie[i])} ",#
      "& {format_val_sd(theta$ffrsusie[i], ssd$cfrsusie[i])} ",#
      "& {format_val_sd(theta$cfrsusie[i], ssd$tfrsusie[i])} \\\\"#
    )#
    line2 <- glue::glue(#
      "&                                 ",#
      "& IP         ",#
      "& - ",#
      "& {format_pip(pip$frsusie[i])} ",#
      "& {format_pip(pip$ffrsusie[i])} ",#
      "& {format_pip(pip$cfrsusie[i])} ",#
      "& {format_pip(pip$tfrsusie[i])} \\\\"#
    )#
    lines <- c(lines, line1, line2)#
  }#
#
  fpr_line <- glue::glue(#
    "& & FPR & {format_cell(rate$mvmrbic[1])} & - & {format_cell(rate$ffrsusie[1])} & {format_cell(rate$cfrsusie[1])} & {format_cell(rate$tfrsusie[1])} \\\\"#
  )#
  fnr_line <- glue::glue(#
    "& & FNR & {format_cell(rate$mvmrbic[2])} & - & {format_cell(rate$ffrsusie[2])} & {format_cell(rate$cfrsusie[2])} & {format_cell(rate$tfrsusie[2])} \\\\"#
  )#
#
  lines <- c(lines, fpr_line, fnr_line)#
  return(lines)#
}#
get_cs_table <- function(file_path) {#
  lines <- readLines(file_path)#
  errors <- as.integer(substr(lines[16], 4, 100))#
  bic <- c(rep("-", 11), errors)#
  # Find section start lines#
  section_starts <- grep("^\\s*set\\s+count\\s*$", lines)#
  stopifnot(length(section_starts) == 4)#
#
  # Helper to clean and parse a section into a data frame#
  clean_section <- function(start, end) {#
    raw_lines <- lines[(start + 1):(end - 1)]#
    parsed <- do.call(rbind, lapply(raw_lines, function(l) {#
      # Remove leading ID column (if exists) and split by whitespace#
      l <- sub("^\\s*\\d+\\s+", "", l)#
      parts <- strcapture("^\\s*(.+?)\\s+(\\d+)\\s*$", l, data.frame(set=character(), count=integer()))#
      return(parts)#
    }))#
    return(parsed)#
  }#
#
  df1 <- clean_section(section_starts[1], section_starts[2])#
  df2 <- clean_section(section_starts[2], section_starts[3])#
  df3 <- clean_section(section_starts[3], section_starts[4])#
  df4 <- clean_section(section_starts[4], length(lines) + 1)#
  frsusie <-  get_table_counts(df1)#
  ffrsusie <-  get_table_counts(df2)#
  cfrsusie <-  get_table_counts(df3)#
  tfrsusie <-  get_table_counts(df4)#
  df <- data.frame(bic=bic, frsusie=frsusie, ffrsusie=ffrsusie, cfrsusie=cfrsusie, tfrsusie=tfrsusie, stringsAsFactors = FALSE)#
  return(format_cs_table(df))#
}#
#
get_2sls_cs_table <- function(file_path) {#
  lines <- readLines(file_path)#
  # Find section start lines#
  section_starts <- grep("^\\s*set\\s+count\\s*$", lines)#
  stopifnot(length(section_starts) == 5)#
#
  # Helper to clean and parse a section into a data frame#
  clean_section <- function(start, end) {#
    raw_lines <- lines[(start + 1):(end - 1)]#
    parsed <- do.call(rbind, lapply(raw_lines, function(l) {#
      # Remove leading ID column (if exists) and split by whitespace#
      l <- sub("^\\s*\\d+\\s+", "", l)#
      parts <- strcapture("^\\s*(.+?)\\s+(\\d+)\\s*$", l, data.frame(set=character(), count=integer()))#
      return(parts)#
    }))#
    return(parsed)#
  }#
  set <- c("(1), (4)", "(1,2), (4)", "(1), (3,4)", "(1,2), (3,4)")#
  df1 <- data.frame(set=set, count=as.numeric(unlist(strsplit(lines[14], "\\s+"))[-1]))#
  df2 <- clean_section(section_starts[1], section_starts[2])#
  df3 <- clean_section(section_starts[2], section_starts[3])#
  df4 <- clean_section(section_starts[3], section_starts[4])#
  df5 <- clean_section(section_starts[4], section_starts[5])#
  df6 <- clean_section(section_starts[5], length(lines) + 1)#
  gift <-  get_2sls_table_counts(df1)#
  susie <-  get_2sls_table_counts(df2)#
  frsusie <-  get_2sls_table_counts(df3)#
  fsusie <-  get_2sls_table_counts(df4)#
  ffrsusie <-  get_2sls_table_counts(df5)#
  cfrsusie <-  get_2sls_table_counts(df6)#
  df <- data.frame(gift=gift, susie=susie, frsusie=frsusie, fsusie=fsusie, ffrsusie=ffrsusie, cfrsusie=cfrsusie, stringsAsFactors = FALSE)#
  return(format_2sls_cs_table(df))#
}#
get_table_counts <- function(df) {#
  sets <- c("(1), (4)", "(1), (4,5)", "(1,2), (4)", "(1,3), (4)",#
    "(1,2), (4,5)", "(1,3), (4,5)", "(1,2,3), (4)", "(1,2,3), (4,5)", "()")#
  counts <- c()#
  for(set in sets){#
    rows <- df[set==df$set, ]#
    counts <- c(counts, ifelse(nrow(rows) > 0, sum(rows$count), 0))#
  }#
  covers <- sum(df[sapply(df$set, if_casual),"count"])#
  counts <- c(counts, 1000-sum(counts), covers, 0)#
  return(counts)#
}#
#
get_2sls_table_counts <- function(df) {#
  sets <- c("(1), (4)", "(1,2), (4)", "(1), (3,4)", "(1,2), (3,4)")#
  counts <- c()#
  for(set in sets){#
    rows <- df[set==df$set, ]#
    counts <- c(counts, ifelse(nrow(rows) > 0, sum(rows$count), 0))#
  }#
  covers <- sum(df[sapply(df$set, if_casual),"count"])#
  counts <- c(counts, 1000-sum(counts), covers)#
  return(counts)#
}#
#
format_2sls_cs_table <- function(df) {#
  labels <- c(#
    "$\\{1\\}, \\{4\\}$",#
    "$\\{1,2\\}, \\{4\\}$",#
    "$\\{1\\}, \\{3,4\\}$",#
    "$\\{1,2\\}, \\{3,4\\}$",#
    "\\# of others",#
    "\\# of covers"#
  )#
#
  stopifnot(nrow(df) == length(labels))#
#
  latex_lines <- character(nrow(df))#
  for (i in 1:nrow(df)) {#
    values <- paste(df[i, ], collapse = " & ")#
    latex_lines[i] <- paste0("& ", labels[i], " & ", values, "\\\\")#
  }#
#
  return(cat(paste(latex_lines, collapse = "\n")))#
}#
format_cs_table <- function(df) {#
  row_labels <- c(#
"$\\{1\\}, \\{4\\}$",#
"$\\{1\\}$, $\\{4,5\\}$",#
"$\\{1,2\\}$, $\\{4\\}$",#
"$\\{1,3\\}$, $\\{4\\}$",#
"$\\{1,2\\}$, $\\{4,5\\}$",#
"$\\{1,3\\}$, $\\{4,5\\}$",#
"$\\{1,2,3\\}$, $\\{4\\}$",#
"$\\{1,2,3\\}$, $\\{4,5\\}$",#
"$\\emptyset$",#
"\\# of others",#
"\\# of covers",#
"\\# of errors"#
  )#
#
  # Check that df has exactly 4 columns:#
  stopifnot(ncol(df) == 5)#
  # Ensure the vector of row labels has the correct length:#
  stopifnot(length(row_labels) == nrow(df))#
  # We'll format:#
  # - The label column left-aligned in a field of 22 characters.#
  # - Each numeric column right-aligned in a field of 6 characters.#
  # If a value is NA, we output a dash ("-").#
  fmt_row <- function(label, vals) {#
    # Format each value: if NA, use "-", otherwise as numeric string.#
    vals_formatted <- sapply(vals, function(x) {#
      if (is.na(x)) "-" else sprintf("%-6s", as.character(x))#
    })#
    # Create the LaTeX line. First column is the label.#
    sprintf("& %-22s & %-6s & %-6s & %-6s & %-6s & %-6s \\\\",#
            label, vals_formatted[1], vals_formatted[2],#
            vals_formatted[3], vals_formatted[4], vals_formatted[5])#
  }#
  # Apply formatting to all rows:#
  latex_rows <- mapply(fmt_row, row_labels, #
                       split(df, seq(nrow(df))),#
                       SIMPLIFY = TRUE)#
  return(cat(paste(latex_rows, collapse = "\n")))#
}#
#
if_casual <- function(input_string) {#
  # Extract all numbers from the string#
  numbers <- as.numeric(unlist(regmatches(input_string, gregexpr("\\d+", input_string))))#
  # Check if 1 or 4 are present#
  all(c(1, 4) %in% numbers)#
}#
#
if_subset <- function(target, collection_vec) {#
  # Helper to extract sets from a string#
  parse_sets <- function(s) {#
    s <- gsub("\\s+", "", s)  # remove spaces#
    sets <- regmatches(s, gregexpr("\\([^\\)]+\\)", s))[[1]]#
    sets#
  }#
  target_sets <- parse_sets(target)#
  sapply(collection_vec, function(full) {#
    full_sets <- parse_sets(full)#
    all(target_sets %in% full_sets)#
  })#
}#
#
get_2sls_table <- function(pip, theta, ssd) {#
  methods <- colnames(pip)#
  n <- nrow(pip)#
#
  latex_lines <- c()#
#
  for (method in methods) {#
    # Extract vectors#
    pip_vec <- pip[[method]]#
    theta_vec <- theta[[method]]#
    ssd_vec <- ssd[[method]]#
#
    # Format: "value (std)"#
    mean_sd <- paste0(#
      sprintf("%.4f", theta_vec), " (", sprintf("%.4f", ssd_vec), ")"#
    )#
#
    # Format PIP line#
    pip_line <- sprintf("%.2f", pip_vec)#
#
    # Begin rows#
    method_label <- switch(method,#
      gift = "GIFT",#
      susie = "SuSiE",#
      frsusie = "frSuSiE",#
      fsusie = "fSuSiE",#
      ffrsusie = "ffrSuSiE",#
      cfrsusie = "cfrSuSiE",#
      method#
    )#
#
    # Compose latex lines#
    mean_row <- paste0("& \\multirow{2}{*}{", method_label, "} & Mean (SSD) & ",#
                       paste(mean_sd, collapse = " & "), "\\\\")#
    pip_row <- paste0("& & IP & ", paste(pip_line, collapse = " & "), "\\\\")#
    latex_lines <- c(latex_lines, mean_row, pip_row)#
  }#
#
  return(latex_lines)#
}
get_est <- function(x, y) {#
  b_hat <- sum(x*y) / sum(x^2)#
  sigma2_hat <- sum((y - b_hat*x)^2)/(length(y)-1)#
  se_hat <- sqrt(sigma2_hat / sum(x^2))#
  return(list(b_hat=b_hat, sigma2_hat=sigma2_hat, se_hat=se_hat))#
}#
#
get_bf <- function(x, y, sigma0=1) {#
  res <- get_est(x,y)#
  b_hat <- res$b_hat#
  sigma2_hat <- res$sigma2_hat#
  s2 <- sigma2_hat / sum(x^2)#
  z <- b_hat / sqrt(s2)#
  BF <- sqrt(s2/(sigma0^2 + s2)) * exp((z^2/2) * (sigma0^2/(sigma0^2+s2)))#
  return(BF)#
}#
#
get_bic <- function(x, y) {#
  res <- get_est(x,y)#
  b_hat <- res$b_hat#
  n <- length(y)#
  BIC <- n*log(sum((y-b_hat*x)^2)/(sum(y^2))) + log(n)#
  return(BIC)#
}#
#
get_pval <- function(x, y) {#
  res <- get_est(x, y)#
  z_score <- res$b_hat / res$se_hat#
  p_val <- 2 * (1 - pt(abs(z_score), length(y)-1))  # two-tailed test#
  return(p_val)#
}#
get_power <- function(x, y, num_sim = 1000, alpha = 0.05) {#
  res <- get_est(x, y)#
  beta_hat <- res$b_hat#
  se_hat <- res$se_hat#
  n <- length(y)#
  reject_count <- 0#
  for (i in 1:num_sim) {#
    # Simulate new Y under H1: Y = X*beta_hat + noise#
    y_sim <- beta_hat * x + rnorm(n, mean = 0, sd = se_hat)#
    # Compute p-value for simulated data#
    p_val <- get_pval(x, y_sim)#
    # Count rejection cases#
    if (p_val < alpha) {#
      reject_count <- reject_count + 1#
    }#
  }#
  # Compute empirical power#
  power_estimate <- reject_count / num_sim#
  return(power_estimate)#
}#
#
get_pip <- function(X, y, beta_mat, pip_mat) {#
  L <- ncol(pip_mat)#
  bic <- numeric(L)#
  n <- length(y)#
  residuals <- y - rowSums(X%*%beta_mat)#
  for(l in 1:L) {#
    r <- residuals + X%*%beta_mat[,l]#
    bic[l] <- n*log(sum(residuals^2)/sum(r^2)) + log(n)#
  }#
#
  res <- remove_null_effect(X, y, beta_mat)#
  include_index <- (bic < 0) & (!res$remove)#
  pip <- 1 - apply(1-pip_mat[,include_index,drop=FALSE], 1, prod)#
  return(list(pip=pip, pvals=res$p_values, include_index=include_index))#
}#
#
remove_null_effect <- function(X, y, beta_mat, p_threshold = 0.05) {#
  n <- length(y)#
  L <- ncol(beta_mat)#
#
  remove_effects <- rep(FALSE, L)#
  delta_sigma2_vec <- rep(NA, L)#
  p_values <- rep(NA, L)#
#
  residuals_full <- y - X %*% rowSums(beta_mat)#
#
  for (l in 1:L) {#
    residuals_partial <- residuals_full + X %*% beta_mat[, l]#
    sigma2_full <- mean(residuals_full^2)#
    sigma2_partial <- mean(residuals_partial^2)#
    # Compute variance reduction#
    delta_sigma2 <- sigma2_partial - sigma2_full#
    # Likelihood Ratio Test (LRT)#
    LRT_stat <- n * (log(sigma2_partial) - log(sigma2_full))#
    p_value <- pchisq(LRT_stat, df = 1, lower.tail = FALSE)#
    # Remove component if variance reduction is negligible or LRT suggests no improvement#
    remove_effects[l] <- (p_value > p_threshold)#
    # Store results#
    delta_sigma2_vec[l] <- delta_sigma2#
    p_values[l] <- p_value#
  }#
  return(list(#
    remove = remove_effects,#
    delta_sigma2 = round(delta_sigma2_vec, 4),#
    p_values = round(p_values, 4)#
   ))#
}#
#
ser <- function(X, y) {#
  p <- ncol(X)#
  n <- length(y)#
#
  bf <- numeric(p)#
  bic <- numeric(p)#
  bf_hat <- numeric(p)#
  beta_hat <- numeric(p)#
  pval_hat <- numeric(p)#
#
  for(j in 1:p) {#
    bf[j] <- get_bf(X[,j], y)#
    bic[j] <- get_bic(X[,j], y)#
#
    beta_hat[j] <- get_est(X[,j], y)$b_hat#
    pval_hat[j] <- get_pval(X[,j], y)#
  }#
#
  # Numerically stable PIP approximation#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  alpha_hat <- bf_hat / sum(bf_hat)#
#
  return(list(alpha_hat=alpha_hat, beta_hat=beta_hat, pval_hat=pval_hat))#
}#
#
get_cs_counts <- function(results) {#
  for(i in 1:length(results)) if(length(results[[i]])==0) results[[i]] <- NA#
  confidence_sets <- sapply(results, function(cs) {#
    # Sort each individual confidence set internally#
    formatted_sets <- sapply(cs, function(set) {#
      if (length(set) == 0) {#
        return("()")  # Handle empty sets explicitly#
      } else {#
        return(paste0("(", paste(sort(set), collapse = ","), ")"))#
      }#
    })#
    # Sort the entire set of confidence sets to ensure consistency#
    paste(sort(formatted_sets), collapse = ", ")#
  })#
  # Count unique confidence sets#
  unique_counts <- table(confidence_sets)#
  # Convert to data frame#
  unique_df <- as.data.frame(unique_counts)#
  colnames(unique_df) <- c("set", "count")#
#
  # Sort by count in descending order for readability#
  unique_df <- unique_df[order(-unique_df$count), ]#
  return(unique_df)#
}#
#
get_vs_counts <- function(beta_matrix) {#
  all_zero <- sum(beta_matrix[,1] == 0 & beta_matrix[,2] == 0)#
  x1_nonzero <- sum(beta_matrix[,1] != 0 & beta_matrix[,2] == 0)#
  x2_nonzero <- sum(beta_matrix[,1] == 0 & beta_matrix[,2] != 0)#
  both_nonzero <- sum(beta_matrix[,1] != 0 & beta_matrix[,2] != 0)#
  return(data.frame(#
    all_zero = all_zero,#
    x1_only = x1_nonzero,#
    x2_only = x2_nonzero,#
    both_nonzero = both_nonzero#
  ))#
}#
#
get_vs2_counts <- function(beta_matrix) {#
  A <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  B <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  C <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  D <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)&(x[6]==0)&(x[7]==0)&(x[8]==0)&(x[9]==0)&(x[10]==0)))#
  out <- c(A,B,C,D)#
  return(out)#
}#
#
get_vs3_counts <- function(beta_matrix) {#
  R23 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]==0)))#
  R134 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]==0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R234 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R123 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]==0)))#
  R124 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]==0)&(x[4]!=0)&(x[5]==0)))#
  R1234 <- sum(apply(beta_matrix, 1, function(x) (x[1]!=0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]==0)))#
  R2345 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]!=0)&(x[5]!=0)))#
  R1235 <- sum(apply(beta_matrix, 1, function(x) (x[1]==0)&(x[2]!=0)&(x[3]!=0)&(x[4]==0)&(x[5]!=0)))#
  out <- c(R23, R134, R234, R123, R124, R1234, R2345, R1235)#
  return(out)#
}#
#
get_vs4_counts <- function(pval_matrix, alpha=0.05) {#
  R134 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R1234 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R34 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R234 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R124 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R14 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R4 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
#
  out <- c(R134, R1234, R34, R234, R124, R14, R4)#
  return(out)#
}#
#
get_vs5_counts <- function(pval_matrix, alpha=0.05) {#
  R14 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R13 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]>alpha)&(x[5]>alpha)))#
  R23 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]>alpha)&(x[5]>alpha)))#
  R24 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
#
  R124 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]>alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R134 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]>alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R234 <- sum(apply(pval_matrix, 1, function(x) (x[1]>alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  R1234 <- sum(apply(pval_matrix, 1, function(x) (x[1]<alpha)&(x[2]<alpha)&(x[3]<alpha)&(x[4]<alpha)&(x[5]>alpha)))#
  out <- c(R14, R124, R134, R1234)#
  return(out)#
}#
reindex_features <- function(X, G) {#
	unique_features <- sort(unique(as.vector(G)))#
	X_subset <- X[, unique_features, drop = FALSE]#
	new_indices <- seq_along(unique_features)  # New indices: 1 to ncol(X_subset)#
	old_to_new <- setNames(new_indices, unique_features)#
	G_subset <- matrix(old_to_new[as.character(as.vector(G))], nrow = 2, byrow = FALSE)#
	return(list(X = X_subset, G = G_subset))#
}#
#
likeli_ratio <- function(y, X, beta, sigma = 1) {#
	(sum((y - X %*% beta)^2) - sum(y^2)) / sigma^2#
}#
#
loss <- function(y, X, beta) {#
	mean((y - X %*% beta)^2)#
}#
#
compute_recovery_rate <- function(beta_hat, beta_true) {#
	true_nonzero_indices <- which(beta_true != 0)#
	hat_nonzero_indices <- which(beta_hat != 0)#
	correct_indices <- intersect(true_nonzero_indices, hat_nonzero_indices)#
	oracle_recovery_rate <- length(correct_indices) / length(true_nonzero_indices) * 100#
	return(oracle_recovery_rate)#
}#
#
mse <- function(x, x_hat) {#
	mean((x-x_hat)^2)#
}#
#
n_in_CS_x = function (x, coverage = 0.95) #
  sum(cumsum(sort(x,decreasing = TRUE)) < coverage) + 1#
#
in_CS_x = function (x, coverage = 0.95) {#
  n = n_in_CS_x(x,coverage)#
  o = order(x,decreasing = TRUE)#
  result = rep(0,length(x))#
  result[o[1:n]] = 1#
  return(result)#
}#
#
in_CS = function (alpha, coverage = 0.95) {#
  #return(filter_status(t(apply(t(alpha), 1, function(x) in_CS_x(x,coverage)))))#
  return(t(apply(t(alpha), 1, function(x) in_CS_x(x,coverage))))#
}#
#
filter_status <- function(mat) {#
  keep <- rep(TRUE, nrow(mat))  # Vector to track rows to keep#
  covered <- rep(0, ncol(mat))  # Track covered columns#
  for (i in 1:nrow(mat)) {#
    ones <- which(mat[i, ] == 1)  # Find columns with 1s in the current row#
    if (any(covered[ones] == 1)) {  # If all 1s are already covered, mark row for removal#
      keep[i] <- FALSE#
    } else {#
      covered[ones] <- 1  # Mark columns as covered#
    }#
  }#
  return(mat[keep, , drop = FALSE])#
}#
#
get_cs <- function(pip_mat, include_index, #
  X=NULL, Xcorr=NULL, check_symmetric=TRUE,#
  min_abs_corr=0.5, n_purity=100, squared=FALSE, #
  coverage = 0.95, use_rfast) {#
#
  if(sum(include_index)==0) {#
    return(list(sets = NULL, coverage = NULL))#
  }#
#
  pip_mat <- pip_mat[, include_index, drop=FALSE]#
#
  status <- in_CS(pip_mat, coverage)#
  cs <- lapply(1:nrow(status), function(i) sort(which(status[i, ] != 0)))#
  # remove duplication#
  unique_cs_strings <- unique(sapply(cs, function(set) paste(set, collapse = ",")))#
  unique_cs <- lapply(unique_cs_strings, function(str) as.integer(unlist(strsplit(str, ","))))#
  claimed_coverage <- sapply(unique_cs, function(set) ifelse(sum(pip_mat[set, ])>1, 1, sum(pip_mat[set, ])))#
  # purity#
  if (!is.null(X) && !is.null(Xcorr))#
    stop("Only one of X or Xcorr should be specified")#
  if (check_symmetric) {#
    if (!is.null(Xcorr) && !is_symmetric_matrix(Xcorr)) {#
      warning_message("Xcorr is not symmetric; forcing Xcorr to be symmetric",#
                  "by replacing Xcorr with (Xcorr + t(Xcorr))/2")#
      Xcorr = Xcorr + t(Xcorr)#
      Xcorr = Xcorr/2#
    }#
  }#
#
  if (is.null(Xcorr) && is.null(X)) {#
    return(list(sets = unique_cs, coverage = claimed_coverage))#
  } else {#
#
    purity = NULL#
    for(idx in 1:length(unique_cs)) {#
      purity <- rbind(purity, matrix(get_purity(unique_cs[[idx]], X, Xcorr, squared, n_purity, use_rfast),1,3))#
    }#
#
    purity = as.data.frame(purity)#
    rownames(purity) <- NULL#
    if (squared) {#
      colnames(purity) = c("min.sq.corr","mean.sq.corr","median.sq.corr")#
    }#
    else {#
      colnames(purity) = c("min.abs.corr","mean.abs.corr","median.abs.corr")#
    }#
#
    threshold = ifelse(squared, min_abs_corr^2, min_abs_corr)#
#
    is_pure = which(purity[,1] >= threshold)#
#
    unique_cs <- unique_cs[is_pure]#
    claimed_coverage <- claimed_coverage[is_pure]#
    purity <- purity[is_pure,]#
    ordering = order(purity[,1],decreasing = TRUE)#
#
    return(list(sets = unique_cs[ordering], #
                coverage = claimed_coverage[ordering],#
                purity   = purity[ordering,]#
                ))#
  }#
}#
#
muffled_corr = function (x)#
  withCallingHandlers(cor(x),#
                      warning = function(w) {#
                        if (grepl("the standard deviation is zero",w$message))#
                          invokeRestart("muffleWarning")#
                      })#
#
get_purity = function (pos, X, Xcorr=NULL, squared = FALSE, n = 100,#
                       use_rfast) {#
  if (missing(use_rfast))#
    use_rfast = requireNamespace("Rfast",quietly = TRUE)#
  if (use_rfast) {#
    # get_upper_tri = Rfast::upper_tri#
    get_upper_tri = function (R) R[upper.tri(R)]#
    get_median    = Rfast::med#
  } else {#
    get_upper_tri = function (R) R[upper.tri(R)]#
    get_median    = stats::median#
  }#
  if (length(pos) == 1)#
    return(c(1,1,1))#
  else {#
#
    # Subsample the columns if necessary.#
    if (length(pos) > n)#
      pos = sample(pos,n)#
#
    if (is.null(Xcorr)) {#
      X_sub = X[,pos]#
      X_sub = as.matrix(X_sub)#
      value = abs(get_upper_tri(muffled_corr(X_sub)))#
    } else#
      value = abs(get_upper_tri(Xcorr[pos,pos]))#
    if (squared)#
      value = value^2#
    return(c(min(value),#
             sum(value)/length(value),#
             get_median(value)))#
  }#
}#
#
fill_vector <- function(vec, p) {#
  setNames(replace(numeric(p), match(names(vec), paste0("X", 1:p)), vec), paste0("X", 1:p))#
}#
#
log_likelihood <- function(X, y, beta_hat, sigma_hat) {#
  n <- length(y)#
  residuals <- y - X %*% beta_hat#
  log_lik <- - (n / 2) * log(2 * pi * sigma_hat^2) - (1 / (2 * sigma_hat^2)) * sum(residuals^2)#
  return(log_lik)#
}#
rmse <- function(x, x_hat) sqrt(mean((x-x_hat)^2))#
#
generate_gamma <- function(num_instruments_block, num_features) {#
  return(matrix(runif(num_instruments_block*num_features, 0.3, 0.5), num_instruments_block, num_features))#
}#
#
generate_alpha <- function(num_instruments, indexes=c(11:15)) {#
  if(length(indexes)>0){#
    alpha <- rnorm(num_instruments, 0.5, 0.5)#
    alpha[-indexes] <- 0#
  }else{#
    alpha <- rep(0, num_instruments)#
  }#
  return(alpha)#
}#
# =====================================================================================#
get_gamma_mvmr <- function(m, p, weight=0.5, K=0){#
  gamma_shared <- runif(m, 0.0, 0.2)#
  gamma <- matrix(NA, m, p)#
  for (j in 1:p) {#
    gamma_specific <- runif(m, 0.2, 0.4)#
    gamma[, j] <- weight * gamma_shared + (1 - weight) * gamma_specific#
  }#
#
  if(K>0) {#
    invalid_set <- sample(1:m, K)#
    gamma[invalid_set, ] <- rep(0, p)#
  }#
#
  return(gamma)#
}#
#
get_pleiotropy <- function(num_invalid, num_instruments, mu_alpha=0, sd_alpha=0.2, min_varphi=0, max_varphi=0.1) {#
  invalid_set <- sample(1:num_instruments, num_invalid)#
  varphi <- rep(0, num_instruments)#
  varphi[invalid_set] <- runif(num_invalid, min_varphi, max_varphi)#
  alpha <- rep(0, num_instruments)#
  alpha[invalid_set] <- rnorm(num_invalid, mu_alpha, sd_alpha)#
  return(list(varphi=varphi, alpha=alpha, invalid_set=invalid_set))#
}#
#
get_gwas <- function(X, Y, Z) {#
  n <- nrow(Z)#
  m <- ncol(Z)#
  p <- ncol(X)#
#
  Z_var <- colSums(Z^2)#
#
  beta_hat_x <- t(Z) %*% X / matrix(Z_var, nrow = m, ncol = p)#
  sigma_hat_x <- matrix(NA, m, p)#
#
  for (j in 1:p) {#
    res_x <- X[, j] - Z %*% beta_hat_x[, j]#
    sigma_hat_x[, j] <- sqrt(colSums(matrix(res_x^2, n, m)) / (n - 2)) / sqrt(Z_var)#
  }#
#
  beta_hat_y <- as.vector((t(Z) %*% Y) / Z_var)#
  res_y <- Y - Z %*% beta_hat_y#
  sigma_hat_y <- sqrt(colSums(matrix(res_y^2, n, m)) / (n - 2)) / sqrt(Z_var)#
#
  return(list(#
    beta_hat_x = beta_hat_x,#
    beta_hat_y = beta_hat_y,#
    sigma_hat_x = sigma_hat_x,#
    sigma_hat_y = sigma_hat_y#
  ))#
}#
#
# MVMR#
get_bic_K <- function(logL, N, K) return(-2*logL + log(N)*K)#
#
get_invcov <- function(sigma_hat_x, sigma_hat_y, rho_hat_xy){#
  m <- nrow(sigma_hat_x)#
  p <- ncol(sigma_hat_x)#
  Sigma_hat <- array(NA, c(p+1, p+1, m))#
  Sigma_inv_hat <- array(NA, c(p+1, p+1, m))#
  for(k in 1:m){#
    Sig <- rho_hat_xy*crossprod(t(c(sigma_hat_x[k,],sigma_hat_y[k])))#
    Sigma_hat[,,k] <- Sig#
    Sigma_inv_hat[,,k] <- solve(Sig)#
  }#
  return(Sigma_inv_hat)#
}#
#
get_beta_tilde_x <- function(theta_tilde, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  beta_tilde_x <- numeric(m)#
  for(k in 1:m) {#
    numerator <- beta_hat_x[k] / sigma_hat_x[k]^2 + theta_tilde*beta_hat_y[k] / sigma_hat_y[k]^2#
    denominator <- 1/sigma_hat_x[k]^2 + theta_tilde^2 / sigma_hat_y[k]^2#
    beta_tilde_x[k] <- numerator / denominator#
  }#
  return(beta_tilde_x)#
}#
#
get_theta_tilde <- function(beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  numerator <- sum(beta_tilde_x * beta_hat_y / sigma_hat_y^2)#
  denominator <- sum(beta_tilde_x^2 / sigma_hat_y^2)#
  theta_tilde <- numerator / denominator#
  return(theta_tilde)#
}#
#
get_uvmr <- function(beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y, max_iter=100) {#
  m <- length(sigma_hat_y)#
  theta_tilde <- 0#
  beta_tilde <- numeric(m)#
  for(iter in 1:max_iter) {#
    beta_tilde_x <- get_beta_tilde_x(theta_tilde, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y)#
    theta_tilde <- get_theta_tilde(beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) #
  }#
  return(list(theta_tilde=theta_tilde, beta_tilde_x=beta_tilde_x))#
}#
#
get_delta_bic <- function(theta_tilde, beta_tilde_x, beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  delta_bic <- sum(((beta_hat_y - theta_tilde * beta_tilde_x)^2 - beta_hat_y^2) / sigma_hat_y^2 + log(m))#
  return(delta_bic)#
}#
#
get_semr <- function(beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y, max_iter=100) {#
  m <- nrow(beta_hat_x)#
  p <- ncol(beta_hat_x)#
  bic <- numeric(p)#
  theta_tilde <- numeric(p)#
  beta_tilde_x <- matrix(NA, m, p)#
  for(j in 1:p) { #
    res_uvmr <- get_uvmr(beta_hat_x[,j], beta_hat_y, sigma_hat_x[,j], sigma_hat_y, max_iter)#
    theta_tilde[j] <- res_uvmr$theta_tilde#
    beta_tilde_x[,j] <- res_uvmr$beta_tilde_x#
    bic[j] <- get_delta_bic(theta_tilde[j], beta_tilde_x[,j], beta_hat_x, beta_hat_y, sigma_hat_x, sigma_hat_y)#
  }#
#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  omega_tilde <- bf_hat / sum(bf_hat)#
  return(list(omega_tilde=omega_tilde, theta_tilde=theta_tilde, beta_tilde_x=beta_tilde_x))#
}#
#
get_ser <- function(y) {#
  m <- length(y)#
  p <- m#
  X <- matrix(0, m, p)#
  diag(X) <- 1#
  bf <- numeric(p)#
  bic <- numeric(p)#
  bf_hat <- numeric(p)#
  beta_hat <- numeric(p)#
  pval_hat <- numeric(p)#
#
  for(j in 1:p) {#
    bf[j] <- get_bf(X[,j], y)#
    bic[j] <- get_bic(X[,j], y)#
#
    beta_hat[j] <- get_est(X[,j], y)$b_hat#
    pval_hat[j] <- get_pval(X[,j], y)#
  }#
#
  # Numerically stable PIP approximation#
  bic_shifted <- bic - min(bic)#
  bf_hat <- exp(-0.5 * bic_shifted)#
  alpha_hat <- bf_hat / sum(bf_hat)#
#
  return(list(alpha_hat=alpha_hat, beta_hat=beta_hat, pval_hat=pval_hat))#
}#
#
get_Sigma_hat <- function(X, sigma_hat_x, sigma_hat_y) {#
  m <- length(sigma_hat_y)#
  p <- ncol(X)#
  Sigma_hat <- array(NA, c(p+1, p+1, m))#
  for(k in 1:m) {#
    Sigma_hat_xk <- matrix(NA, p, p)#
    rho_hat_x <- cor(X)#
    for(i in 1:p){#
      for(j in 1:p){#
        Sigma_hat_xk[i,j] <- rho_hat_x[i,j] * sigma_hat_x[k,i] * sigma_hat_x[k,j]#
      }#
    }#
    Sigma_hat_xyk <- matrix(0, p, 1)#
    Sigma_hat_yxk <- t(Sigma_hat_xyk)#
    Sigma_hat[,,k] <- rbind(cbind(Sigma_hat_xk, Sigma_hat_xyk), #
      c(Sigma_hat_yxk, sigma_hat_y[k]^2))#
  }#
  return(Sigma_hat)#
}#
#
get_log_likelihood <- function(theta_hat, beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, invSigma_hat, nugget=0) {#
  m <- length(beta_hat_y)#
  p <- ncol(beta_tilde_x)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
  out <- 0#
  for(k in 1:m) {#
    beta_hat_k <- c(beta_hat_x[k,], beta_hat_y[k])#
    beta_tilde_k <- c(beta_tilde_x[k,], sum(theta_hat * beta_tilde_x[k,])+r_tilde[k])#
    invSigma_hat_k <- invSigma_hat[,,k]#
    out <- out + t(beta_hat_k - beta_tilde_k) %*% invSigma_hat_k %*% (beta_hat_k - beta_tilde_k)#
  }#
  return(-0.5 * out)#
}#
#
remove_null_mvmr <- function(theta_tilde, beta_tilde_x, r_tilde,#
  beta_hat_x, beta_hat_y, Sigma_hat, nugget=0, threshold = 0.05) {#
  m <- length(beta_hat_y)#
  L <- ncol(theta_tilde)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
  log_lik_full <- get_log_likelihood(rowSums(theta_tilde), beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget)#
  remove_effects <- numeric(L)#
  p_values <- numeric(L)#
  for (l in 1:L) {#
  log_lik_null <- get_log_likelihood(rowSums(theta_tilde[,-l]), beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget)#
    LRT_stat <- -2 * (log_lik_full - log_lik_null) #
    p_value <- pchisq(LRT_stat, df = p, lower.tail = FALSE)#
    # Remove component if variance reduction is negligible or LRT suggests no improvement#
    remove_effects[l] <- (p_value > threshold)#
    p_values[l] <- p_value#
  }#
  return(list(#
    remove = remove_effects,#
    p_values = round(p_values, 4)#
   ))#
}#
kl_divergence <- function(p, base = exp(1)) {#
  q <- rep(1/length(p), length(p))#
  nonzero <- p > 0#
  distance <- sum(p[nonzero] * log(p[nonzero] / q[nonzero], base = base))#
  return(distance)#
}#
#
get_loglik_k <- function(theta_hat, beta_tilde_xk, r_tilde_k, #
  beta_hat_xk, beta_hat_yk, Sigma_inv_hat_k) {#
  beta_hat_k <- c(beta_hat_xk, beta_hat_yk)#
  beta_tilde_k <- c(beta_tilde_xk, sum(beta_tilde_xk * theta_hat) + r_tilde_k)#
  ell_k <- -0.5 * t(beta_hat_k - beta_tilde_k) %*% Sigma_inv_hat_k %*% (beta_hat_k - beta_tilde_k)#
  return(ell_k)#
}#
get_pip_mvmr <- function(omega_tilde, theta_tilde, beta_tilde_x, r_tilde,#
  beta_hat_x, beta_hat_y, Sigma_hat, nugget=0, threshold=0.1) {#
  m <- length(beta_hat_y)#
  L <- ncol(theta_tilde)#
  p <- ncol(beta_tilde_x)#
  if(sum(r_tilde)==0) r_tilde <- rep(0, m)#
#
  # res <- remove_null_mvmr(theta_tilde, beta_tilde_x, r_tilde, beta_hat_x, beta_hat_y, Sigma_hat, nugget, threshold)#
  #include_index <- (bic < 0) & (!res$remove)#
  #include_index <- !res$remove#
  dists <- apply(omega_tilde, 2, kl_divergence)#
  include_index <- (dists > threshold)#
  if(all(!include_index)) include_index[which.max(dists)] <- TRUE#
#
  pip <- 1 - apply(1-omega_tilde[,include_index,drop=FALSE], 1, prod)#
  return(list(pip=pip, dists=dists, include_index=include_index))#
}#
#
get_invalid_rates <- function(estimated, truth) {#
  estimated <- sort(unique(estimated))#
  truth <- sort(unique(truth))#
  tp <- length(intersect(estimated, truth))#
  fp <- length(setdiff(estimated, truth))#
  fn <- length(setdiff(truth, estimated))#
  precision <- if ((tp + fp) == 0) NA else tp / (tp + fp)#
  recall <- if ((tp + fn) == 0) NA else tp / (tp + fn)#
  f1 <- if (is.na(precision) || is.na(recall) || (precision + recall) == 0) NA else#
    2 * precision * recall / (precision + recall)#
  fpr <- if (length(estimated) == 0) NA else fp / length(estimated)#
  fnr <- if (length(truth) == 0) NA else fn / length(truth)#
  return(c(fpr, fnr))#
}#
#
get_W <- function(Sigma_inv_hat) {#
  dims <- dim(Sigma_inv_hat)#
  m <- dims[3]#
  p <- dims[1]#
  W_mat <- matrix(NA, nrow = m, ncol = p)#
  W_vec <- numeric(m)#
#
  for (k in 1:m) {#
    W_mat[k, ] <- Sigma_inv_hat[p,,k]         # last row#
    W_vec[k] <- Sigma_inv_hat[p,p,k]          # bottom-right element#
  }#
#
  return(list(mat = W_mat, vec = W_vec))#
}#
#
get_table <- function(theta, ssd, pip, rate) {#
  stopifnot(nrow(theta) == 5, nrow(ssd) == 5, nrow(pip) == 5)#
  beta_labels <- c("$\\beta_1=0.5$", "$\\beta_2=0$", "$\\beta_3=0$", "$\\beta_4=0.5$", "$\\beta_5=0$")#
  format_val_sd <- function(val, sd) {#
    paste0(formatC(val, digits = 4, format = "f", width = 7), #
           " (", formatC(sd, digits = 2, format = "f", width = 4), ")")#
  }#
  format_pip <- function(x) {#
    if (is.na(x)) return("-")#
    formatC(x, digits = 2, format = "f", width = 6)#
  }#
#
  format_cell <- function(x) {#
    if (is.na(x)) return("-")#
    formatC(x, digits = 4, format = "f", width = 7)#
  }#
#
  lines <- character()#
#
  for (i in 1:5) {#
    line1 <- glue::glue(#
      "& \\multirow{{2}}{{*}}{{{beta_labels[i]}}} ",#
      "& Mean (SSD) ",#
      "& {format_val_sd(theta$mvmrbic[i], ssd$mvmrbic[i])} ",#
      "& {format_val_sd(theta$frsusie[i], ssd$frsusie[i])} ",#
      "& {format_val_sd(theta$ffrsusie[i], ssd$ffrsusie[i])} ",#
      "& {format_val_sd(theta$ffrsusie[i], ssd$cfrsusie[i])} ",#
      "& {format_val_sd(theta$cfrsusie[i], ssd$tfrsusie[i])} \\\\"#
    )#
    line2 <- glue::glue(#
      "&                                 ",#
      "& IP         ",#
      "& - ",#
      "& {format_pip(pip$frsusie[i])} ",#
      "& {format_pip(pip$ffrsusie[i])} ",#
      "& {format_pip(pip$cfrsusie[i])} ",#
      "& {format_pip(pip$tfrsusie[i])} \\\\"#
    )#
    lines <- c(lines, line1, line2)#
  }#
#
  fpr_line <- glue::glue(#
    "& & FPR & {format_cell(rate$mvmrbic[1])} & - & {format_cell(rate$ffrsusie[1])} & {format_cell(rate$cfrsusie[1])} & {format_cell(rate$tfrsusie[1])} \\\\"#
  )#
  fnr_line <- glue::glue(#
    "& & FNR & {format_cell(rate$mvmrbic[2])} & - & {format_cell(rate$ffrsusie[2])} & {format_cell(rate$cfrsusie[2])} & {format_cell(rate$tfrsusie[2])} \\\\"#
  )#
#
  lines <- c(lines, fpr_line, fnr_line)#
  return(lines)#
}#
get_cs_table <- function(file_path) {#
  lines <- readLines(file_path)#
  errors <- as.integer(substr(lines[16], 4, 100))#
  bic <- c(rep("-", 11), errors)#
  # Find section start lines#
  section_starts <- grep("^\\s*set\\s+count\\s*$", lines)#
  stopifnot(length(section_starts) == 4)#
#
  # Helper to clean and parse a section into a data frame#
  clean_section <- function(start, end) {#
    raw_lines <- lines[(start + 1):(end - 1)]#
    parsed <- do.call(rbind, lapply(raw_lines, function(l) {#
      # Remove leading ID column (if exists) and split by whitespace#
      l <- sub("^\\s*\\d+\\s+", "", l)#
      parts <- strcapture("^\\s*(.+?)\\s+(\\d+)\\s*$", l, data.frame(set=character(), count=integer()))#
      return(parts)#
    }))#
    return(parsed)#
  }#
#
  df1 <- clean_section(section_starts[1], section_starts[2])#
  df2 <- clean_section(section_starts[2], section_starts[3])#
  df3 <- clean_section(section_starts[3], section_starts[4])#
  df4 <- clean_section(section_starts[4], length(lines) + 1)#
  frsusie <-  get_table_counts(df1)#
  ffrsusie <-  get_table_counts(df2)#
  cfrsusie <-  get_table_counts(df3)#
  tfrsusie <-  get_table_counts(df4)#
  df <- data.frame(bic=bic, frsusie=frsusie, ffrsusie=ffrsusie, cfrsusie=cfrsusie, tfrsusie=tfrsusie, stringsAsFactors = FALSE)#
  return(format_cs_table(df))#
}#
#
get_2sls_cs_table <- function(file_path) {#
  lines <- readLines(file_path)#
  # Find section start lines#
  section_starts <- grep("^\\s*set\\s+count\\s*$", lines)#
  stopifnot(length(section_starts) == 5)#
#
  # Helper to clean and parse a section into a data frame#
  clean_section <- function(start, end) {#
    raw_lines <- lines[(start + 1):(end - 1)]#
    parsed <- do.call(rbind, lapply(raw_lines, function(l) {#
      # Remove leading ID column (if exists) and split by whitespace#
      l <- sub("^\\s*\\d+\\s+", "", l)#
      parts <- strcapture("^\\s*(.+?)\\s+(\\d+)\\s*$", l, data.frame(set=character(), count=integer()))#
      return(parts)#
    }))#
    return(parsed)#
  }#
  set <- c("(1), (4)", "(1,2), (4)", "(1), (3,4)", "(1,2), (3,4)")#
  df1 <- data.frame(set=set, count=as.numeric(unlist(strsplit(lines[14], "\\s+"))[-1]))#
  df2 <- clean_section(section_starts[1], section_starts[2])#
  df3 <- clean_section(section_starts[2], section_starts[3])#
  df4 <- clean_section(section_starts[3], section_starts[4])#
  df5 <- clean_section(section_starts[4], section_starts[5])#
  df6 <- clean_section(section_starts[5], length(lines) + 1)#
  gift <-  get_2sls_table_counts(df1)#
  susie <-  get_2sls_table_counts(df2)#
  frsusie <-  get_2sls_table_counts(df3)#
  fsusie <-  get_2sls_table_counts(df4)#
  ffrsusie <-  get_2sls_table_counts(df5)#
  cfrsusie <-  get_2sls_table_counts(df6)#
  df <- data.frame(gift=gift, susie=susie, frsusie=frsusie, fsusie=fsusie, ffrsusie=ffrsusie, cfrsusie=cfrsusie, stringsAsFactors = FALSE)#
  return(format_2sls_cs_table(df))#
}#
get_table_counts <- function(df) {#
  sets <- c("(1), (4)", "(1), (4,5)", "(1,2), (4)", "(1,3), (4)",#
    "(1,2), (4,5)", "(1,3), (4,5)", "(1,2,3), (4)", "(1,2,3), (4,5)", "()")#
  counts <- c()#
  for(set in sets){#
    rows <- df[set==df$set, ]#
    counts <- c(counts, ifelse(nrow(rows) > 0, sum(rows$count), 0))#
  }#
  covers <- sum(df[sapply(df$set, if_casual),"count"])#
  counts <- c(counts, 1000-sum(counts), covers, 0)#
  return(counts)#
}#
#
get_2sls_table_counts <- function(df) {#
  sets <- c("(1), (4)", "(1,2), (4)", "(1), (3,4)", "(1,2), (3,4)")#
  counts <- c()#
  for(set in sets){#
    rows <- df[set==df$set, ]#
    counts <- c(counts, ifelse(nrow(rows) > 0, sum(rows$count), 0))#
  }#
  covers <- sum(df[sapply(df$set, if_casual),"count"])#
  counts <- c(counts, 1000-sum(counts), covers)#
  return(counts)#
}#
#
format_2sls_cs_table <- function(df) {#
  labels <- c(#
    "$\\{1\\}, \\{4\\}$",#
    "$\\{1,2\\}, \\{4\\}$",#
    "$\\{1\\}, \\{3,4\\}$",#
    "$\\{1,2\\}, \\{3,4\\}$",#
    "\\# of others",#
    "\\# of covers"#
  )#
#
  stopifnot(nrow(df) == length(labels))#
#
  latex_lines <- character(nrow(df))#
  for (i in 1:nrow(df)) {#
    values <- paste(df[i, ], collapse = " & ")#
    latex_lines[i] <- paste0("& ", labels[i], " & ", values, "\\\\")#
  }#
#
  return(cat(paste(latex_lines, collapse = "\n")))#
}#
format_cs_table <- function(df) {#
  row_labels <- c(#
"$\\{1\\}, \\{4\\}$",#
"$\\{1\\}$, $\\{4,5\\}$",#
"$\\{1,2\\}$, $\\{4\\}$",#
"$\\{1,3\\}$, $\\{4\\}$",#
"$\\{1,2\\}$, $\\{4,5\\}$",#
"$\\{1,3\\}$, $\\{4,5\\}$",#
"$\\{1,2,3\\}$, $\\{4\\}$",#
"$\\{1,2,3\\}$, $\\{4,5\\}$",#
"$\\emptyset$",#
"\\# of others",#
"\\# of covers",#
"\\# of errors"#
  )#
#
  # Check that df has exactly 4 columns:#
  stopifnot(ncol(df) == 5)#
  # Ensure the vector of row labels has the correct length:#
  stopifnot(length(row_labels) == nrow(df))#
  # We'll format:#
  # - The label column left-aligned in a field of 22 characters.#
  # - Each numeric column right-aligned in a field of 6 characters.#
  # If a value is NA, we output a dash ("-").#
  fmt_row <- function(label, vals) {#
    # Format each value: if NA, use "-", otherwise as numeric string.#
    vals_formatted <- sapply(vals, function(x) {#
      if (is.na(x)) "-" else sprintf("%-6s", as.character(x))#
    })#
    # Create the LaTeX line. First column is the label.#
    sprintf("& %-22s & %-6s & %-6s & %-6s & %-6s & %-6s \\\\",#
            label, vals_formatted[1], vals_formatted[2],#
            vals_formatted[3], vals_formatted[4], vals_formatted[5])#
  }#
  # Apply formatting to all rows:#
  latex_rows <- mapply(fmt_row, row_labels, #
                       split(df, seq(nrow(df))),#
                       SIMPLIFY = TRUE)#
  return(cat(paste(latex_rows, collapse = "\n")))#
}#
#
if_casual <- function(input_string) {#
  # Extract all numbers from the string#
  numbers <- as.numeric(unlist(regmatches(input_string, gregexpr("\\d+", input_string))))#
  # Check if 1 or 4 are present#
  all(c(1, 4) %in% numbers)#
}#
#
if_subset <- function(target, collection_vec) {#
  # Helper to extract sets from a string#
  parse_sets <- function(s) {#
    s <- gsub("\\s+", "", s)  # remove spaces#
    sets <- regmatches(s, gregexpr("\\([^\\)]+\\)", s))[[1]]#
    sets#
  }#
  target_sets <- parse_sets(target)#
  sapply(collection_vec, function(full) {#
    full_sets <- parse_sets(full)#
    all(target_sets %in% full_sets)#
  })#
}#
#
get_2sls_table <- function(pip, theta, ssd) {#
  methods <- colnames(pip)#
  n <- nrow(pip)#
#
  latex_lines <- c()#
#
  for (method in methods) {#
    # Extract vectors#
    pip_vec <- pip[[method]]#
    theta_vec <- theta[[method]]#
    ssd_vec <- ssd[[method]]#
#
    # Format: "value (std)"#
    mean_sd <- paste0(#
      sprintf("%.4f", theta_vec), " (", sprintf("%.4f", ssd_vec), ")"#
    )#
#
    # Format PIP line#
    pip_line <- sprintf("%.2f", pip_vec)#
#
    # Begin rows#
    method_label <- switch(method,#
      gift = "GIFT",#
      susie = "SuSiE",#
      frsusie = "frSuSiE",#
      fsusie = "fSuSiE",#
      ffrsusie = "ffrSuSiE",#
      cfrsusie = "cfrSuSiE",#
      method#
    )#
#
    # Compose latex lines#
    mean_row <- paste0("& \\multirow{2}{*}{", method_label, "} & Mean (SSD) & ",#
                       paste(mean_sd, collapse = " & "), "\\\\")#
    pip_row <- paste0("& & IP & ", paste(pip_line, collapse = " & "), "\\\\")#
    latex_lines <- c(latex_lines, mean_row, pip_row)#
  }#
#
  return(latex_lines)#
}
get_bic(X[,j], r_j)
get_bic
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    bic_shifted <- bic - min(bic)#
    bf_hat <- exp(-0.5 * bic_shifted)#
    alpha_hat <- bf_hat / sum(bf_hat)#
#
    beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    bic_shifted <- bic - min(bic)#
    bf_hat <- exp(-0.5 * bic_shifted)#
    alpha_hat <- bf_hat / sum(bf_hat)#
#
    beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    bic_shifted <- bic - min(bic)#
    bf_hat <- exp(-0.5 * bic_shifted)#
    alpha_hat <- bf_hat / sum(bf_hat)#
#
    beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 1), c(1, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    bic_shifted <- bic - min(bic)#
    bf_hat <- exp(-0.5 * bic_shifted)#
    alpha_hat <- bf_hat / sum(bf_hat)#
#
    beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 1), c(1, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
lambda=1
tol = 1e-6
max_iter = 1000
n <- nrow(X)
p <- ncol(X)
# Center X and Y
X <- scale(X, center = TRUE, scale = FALSE)
Y <- scale(Y, center = TRUE, scale = FALSE)
beta <- rep(0, p)
bic <- rep(0, p)
for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    bic_shifted <- bic - min(bic)#
    bf_hat <- exp(-0.5 * bic_shifted)#
    alpha_hat <- bf_hat / sum(bf_hat)#
#
    beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }
beta
round(beta,2)
alpha_hat
round(alpha_hat,2)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 1), c(1, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
theta[4] <- 2
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 2#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 1#
theta[2] <- 0#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 2#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.98), c(0.98, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 2#
theta[3] <- 0#
theta[4] <- 4#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 500#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 2#
theta[3] <- 0#
theta[4] <- 4#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 2#
theta[3] <- 0#
theta[4] <- 4#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 10#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 20#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 25#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 30#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 200#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 2)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 200#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 0.5)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    raw_w <- (abs(beta) + epsilon)^gamma#
    w <- tau * raw_w / sum(raw_w)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 200#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 1, gamma = 0.1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
pip_alasso <- (weight_alasso>0)*1#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, pip_alasso)
pip_susie <- round(res_susie$pip, 2)
weight_alasso <- round(res_slasso$weights,2)
pip_alasso <- (weight_alasso>0)*1
cbind(beta_susie, beta_alasso)
cbind(pip_susie, pip_alasso, weight_alasso)
cbind(pip_susie, weight_alasso)
pip_susie <- round(res_susie$pip, 2)
weight_alasso <- round(res_slasso$weights,2)
cbind(beta_susie, beta_alasso)
cbind(pip_susie, weight_alasso)
n <- 200#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = p, gamma = 0.1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    for(j in 1:p) {#
      r <- Y - X%*%beta#
      bic[j] <- get_bic(X[,j], r)#
      bf_hat[j] <- exp(-0.5*bic[j])#
    }#
    alpha <- numeric(p)#
    for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    #alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
n <- 100#
Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
y <- X %*% beta_true + rnorm(n)#
#
res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
round(res$beta, 3)#
round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    for(j in 1:p) {#
      r <- Y - X%*%beta#
      bic[j] <- get_bic(X[,j], r)#
      bf_hat[j] <- exp(-0.5*bic[j])#
    }#
    alpha <- numeric(p)#
    for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    #alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # for(j in 1:p) {#
    #   r <- Y - X%*%beta#
    #   bic[j] <- get_bic(X[,j], r)#
    #   bf_hat[j] <- exp(-0.5*bic[j])#
    # }#
    # alpha <- numeric(p)#
    # for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    #alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # for(j in 1:p) {#
    #   r <- Y - X%*%beta#
    #   bic[j] <- get_bic(X[,j], r)#
    #   bf_hat[j] <- exp(-0.5*bic[j])#
    # }#
    # alpha <- numeric(p)#
    # for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Center X and Y#
  X <- scale(X, center = TRUE, scale = FALSE)#
  Y <- scale(Y, center = TRUE, scale = FALSE)#
#
  beta <- rep(0, p)#
  bic <- rep(0, p)#
  for (iter in 1:max_iter) {#
    beta_old <- beta#
    for (j in 1:p) {#
      # Partial residual excluding j#
      r_j <- Y - X[, -j] %*% beta[-j]#
      rho_j <- sum(X[, j] * r_j)#
      bic[j] <- get_bic(X[,j], r_j)#
      # Soft-thresholding#
      if (rho_j < -lambda) {#
        beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
      } else if (rho_j > lambda) {#
        beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
      } else {#
        beta[j] <- 0#
      }#
    }#
#
    # bic_shifted <- bic - min(bic)#
    # bf_hat <- exp(-0.5 * bic_shifted)#
    # alpha_hat <- bf_hat / sum(bf_hat)#
#
    # beta <- alpha_hat * beta#
#
    # Check convergence#
    if (sum(abs(beta - beta_old)) < tol) break#
  }#
#
  return(beta)#
}#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # for(j in 1:p) {#
    #   r <- Y - X%*%beta#
    #   bic[j] <- get_bic(X[,j], r)#
    #   bf_hat[j] <- exp(-0.5*bic[j])#
    # }#
    # alpha <- numeric(p)#
    # for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
# lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
#   n <- nrow(X)#
#   p <- ncol(X)#
#
#   # Center X and Y#
#   X <- scale(X, center = TRUE, scale = FALSE)#
#   Y <- scale(Y, center = TRUE, scale = FALSE)#
#
#   beta <- rep(0, p)#
#   bic <- rep(0, p)#
#   for (iter in 1:max_iter) {#
#     beta_old <- beta#
#     for (j in 1:p) {#
#       # Partial residual excluding j#
#       r_j <- Y - X[, -j] %*% beta[-j]#
#       rho_j <- sum(X[, j] * r_j)#
#       bic[j] <- get_bic(X[,j], r_j)#
#       # Soft-thresholding#
#       if (rho_j < -lambda) {#
#         beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
#       } else if (rho_j > lambda) {#
#         beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
#       } else {#
#         beta[j] <- 0#
#       }#
#     }#
#
#     # bic_shifted <- bic - min(bic)#
#     # bf_hat <- exp(-0.5 * bic_shifted)#
#     # alpha_hat <- bf_hat / sum(bf_hat)#
#
#     # beta <- alpha_hat * beta#
#
#     # Check convergence#
#     if (sum(abs(beta - beta_old)) < tol) break#
#   }#
#
#   return(beta)#
# }#
#
res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # for(j in 1:p) {#
    #   r <- Y - X%*%beta#
    #   bic[j] <- get_bic(X[,j], r)#
    #   bf_hat[j] <- exp(-0.5*bic[j])#
    # }#
    # alpha <- numeric(p)#
    # for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
# lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
#   n <- nrow(X)#
#   p <- ncol(X)#
#
#   # Center X and Y#
#   X <- scale(X, center = TRUE, scale = FALSE)#
#   Y <- scale(Y, center = TRUE, scale = FALSE)#
#
#   beta <- rep(0, p)#
#   bic <- rep(0, p)#
#   for (iter in 1:max_iter) {#
#     beta_old <- beta#
#     for (j in 1:p) {#
#       # Partial residual excluding j#
#       r_j <- Y - X[, -j] %*% beta[-j]#
#       rho_j <- sum(X[, j] * r_j)#
#       bic[j] <- get_bic(X[,j], r_j)#
#       # Soft-thresholding#
#       if (rho_j < -lambda) {#
#         beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
#       } else if (rho_j > lambda) {#
#         beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
#       } else {#
#         beta[j] <- 0#
#       }#
#     }#
#
#     # bic_shifted <- bic - min(bic)#
#     # bf_hat <- exp(-0.5 * bic_shifted)#
#     # alpha_hat <- bf_hat / sum(bf_hat)#
#
#     # beta <- alpha_hat * beta#
#
#     # Check convergence#
#     if (sum(abs(beta - beta_old)) < tol) break#
#   }#
#
#   return(beta)#
# }#
#
#res_lasso <- lasso(X, Y, 1)#
#
n <- 50#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
library(glmnet)#
# n <- 500#
# p <- 4#
# X <- matrix(rnorm(n*p), n, p)#
# X[,1] = X[,2]#
# X[,3] = X[,4]#
# beta <- c(1,0,0,1)#
# y <- X%*%beta + rnorm(n)#
#
# adaptive_lasso <- glmnet(#
#   x = X, #
#   y = y, #
#   alpha = 1, #
#   penalty.factor = 1 / abs(best_ridge_coef))#
#
soft_adaptive_lasso <- function(X, Y, lambda = NULL, tau = 1, gamma = 1, max_iter = 20, tol = 1e-4) {#
  library(glmnet)#
  n <- nrow(X)#
  p <- ncol(X)#
#
  # Initialize weights uniformly#
  w <- rep(tau / p, p)#
  beta_prev <- rep(0, p)#
  bic <- rep(0, p)#
  bf_hat <- rep(0, p)#
  for (iter in 1:max_iter) {#
    # Fit weighted Lasso#
    if (is.null(lambda)) {#
      cvfit <- cv.glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, standardize = FALSE)#
      lambda_used <- cvfit$lambda.min#
      beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
    } else {#
      fit <- glmnet(X, Y, alpha = 1, penalty.factor = 1 / w, lambda = lambda, standardize = FALSE)#
      beta <- as.vector(coef(fit))[-1]#
      lambda_used <- lambda#
    }#
#
    # for(j in 1:p) {#
    #   r <- Y - X%*%beta#
    #   bic[j] <- get_bic(X[,j], r)#
    #   bf_hat[j] <- exp(-0.5*bic[j])#
    # }#
    # alpha <- numeric(p)#
    # for(j in 1:p) alpha[j] <- bf_hat[j]/sum(bf_hat)#
#
    # Check convergence#
    if (sum(abs(beta - beta_prev)) < tol) break#
    beta_prev <- beta#
#
    # Update weights softly#
    epsilon <- 1e-6#
    alpha <- (abs(beta) + epsilon)^gamma#
    w <- tau * alpha / sum(alpha)#
  }#
#
  return(list(beta = beta, weights = w, lambda = lambda_used))#
}#
#
# lasso <- function(X, Y, lambda=1, tol = 1e-6, max_iter = 1000) {#
#   n <- nrow(X)#
#   p <- ncol(X)#
#
#   # Center X and Y#
#   X <- scale(X, center = TRUE, scale = FALSE)#
#   Y <- scale(Y, center = TRUE, scale = FALSE)#
#
#   beta <- rep(0, p)#
#   bic <- rep(0, p)#
#   for (iter in 1:max_iter) {#
#     beta_old <- beta#
#     for (j in 1:p) {#
#       # Partial residual excluding j#
#       r_j <- Y - X[, -j] %*% beta[-j]#
#       rho_j <- sum(X[, j] * r_j)#
#       bic[j] <- get_bic(X[,j], r_j)#
#       # Soft-thresholding#
#       if (rho_j < -lambda) {#
#         beta[j] <- (rho_j + lambda) / sum(X[, j]^2)#
#       } else if (rho_j > lambda) {#
#         beta[j] <- (rho_j - lambda) / sum(X[, j]^2)#
#       } else {#
#         beta[j] <- 0#
#       }#
#     }#
#
#     # bic_shifted <- bic - min(bic)#
#     # bf_hat <- exp(-0.5 * bic_shifted)#
#     # alpha_hat <- bf_hat / sum(bf_hat)#
#
#     # beta <- alpha_hat * beta#
#
#     # Check convergence#
#     if (sum(abs(beta - beta_old)) < tol) break#
#   }#
#
#   return(beta)#
# }#
#
#res_lasso <- lasso(X, Y, 1)#
#
n <-200#
p <- 4#
Sigma <- rbind(c(1, 0.99), c(0.99, 1))#
theta <- rep(0, p)#
theta[1] <- 0#
theta[2] <- 1#
theta[3] <- 0#
theta[4] <- 1#
#
X12 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X34 <- mvtnorm::rmvnorm(n, sigma=Sigma)#
X <- cbind(X12, X34)#
Y <- X %*% theta + rnorm(n)#
#
Y <- scale(Y, center=TRUE, scale=FALSE)#
X <- scale(X, center=TRUE, scale=FALSE)#
#
res_slasso <- soft_adaptive_lasso(X, Y, tau = 2, gamma = 1)#
res_susie <- susieR::susie(X, Y, L=10)#
#
# cvfit <- cv.glmnet(X, Y, alpha = 0, standardize = FALSE)#
# lambda_used <- cvfit$lambda.min#
# beta <- as.vector(coef(cvfit, s = "lambda.min"))[-1]#
#
beta_susie <- round(coef(res_susie)[-1],2)#
beta_alasso <- round(res_slasso$beta,2)#
#
pip_susie <- round(res_susie$pip, 2)#
weight_alasso <- round(res_slasso$weights,2)#
cbind(beta_susie, beta_alasso)#
cbind(pip_susie, weight_alasso)#
#
# n <- 100#
# Sigma <- matrix(c(1, 0.99, 0.99, 1), 2, 2)#
# X12 <- mvtnorm::rmvnorm(n, sigma = Sigma)#
# X <- scale(cbind(X12, matrix(rnorm(n*8), n, 8)), center = TRUE, scale = FALSE)#
# beta_true <- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)#
# y <- X %*% beta_true + rnorm(n)#
#
# res <- one_step_soft_selection(X, y, lambda = 1, tau = 1)#
# round(res$beta, 3)#
# round(res$alpha, 3)
#' Fit Single-Effect Regression (SER) Across Predictors#
#'#
#' @description#
#' For each column of \code{X}, fit a univariate model (GLM or Cox) against#
#' \code{y}, compute the BIC difference versus the null model, and return#
#' approximate Bayes factors and normalized posterior model probabilities.#
#'#
#' @param X Numeric matrix (n × p) or numeric vector (length n).#
#' @param y Response variable.#
#'   \itemize{#
#'     \item For GLM: a numeric vector or a two-column matrix (for binomial).#
#'     \item For Cox model: a n × 2 matrix (time, status).#
#'   }#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or the string \code{"cox"}.#
#' @param offset Numeric scalar or length-n vector. Offset on the linear predictor scale (default \code{0}).#
#' @param standardize Logical(1). If \code{TRUE}, center and scale each predictor by its L2 norm (default \code{TRUE}).#
#' @param ties Character(1). Tie-handling method for Cox models; one of \code{"efron"}, \code{"breslow"}, #
#'        \code{"exact"}, \code{"user"}. Ignored for GLMs. Default is \code{"efron"}.#
#' @param parallel Logical(1). If \code{TRUE}, use parallel computing via \code{mclapply} (default \code{TRUE}).#
#'#
#' @return A \code{data.frame} with one row per predictor and columns:#
#' \describe{#
#'   \item{theta}{Estimated slope.}#
#'   \item{se}{Standard error of \code{theta}.}#
#'   \item{p_value}{Wald test p-value.}#
#'   \item{logLik}{Model (partial) log-likelihood.}#
#'   \item{BIC}{Bayesian Information Criterion.}#
#'   \item{delta_BIC}{BIC difference from the best model (lower is better).}#
#'   \item{bf}{Approximate Bayes factor.}#
#'   \item{posterior}{Normalized posterior model probability.}#
#' }#
#'#
#' @examples#
#' \dontrun{#
# ' # Gaussian GLM example (parallel)#
# ' set.seed(1)#
# ' X <- matrix(rnorm(100 * 5), ncol = 5)#
# ' y <- X[,2] + rnorm(100)#
# ' res_glm <- get_ser_fit(X, y, family = gaussian(), parallel = TRUE)#
# ' print(res_glm)#
#'#
#' # Cox proportional hazards example (parallel)#
#' library(survival)#
#' set.seed(2)#
#' n <- 100#
#' X <- matrix(rnorm(n * 3), ncol = 3)#
#' time <- rexp(n, rate = exp(X[,1]))#
#' status <- rbinom(n, 1, 0.7)#
#' y_cox <- cbind(time, status)#
#' res_cox <- get_ser_fit(X, y_cox, family = "cox", parallel = TRUE)#
#' print(res_cox)#
#' }#
#'#
#' @seealso #
#' \code{\link{get_glm_fit}} for single predictor GLM fitting,#
#' \code{\link{get_cox_fit}} for single predictor Cox model fitting,#
#' \code{\link[parallel]{mclapply}} for parallel processing#
#'#
#' @importFrom parallel detectCores mclapply#
#' @import survival#
#' @export#
get_ser_fit <- function(#
  X,#
  y,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE#
) {#
  ## Validate X#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
  n <- nrow(X)#
  p <- ncol(X)#
#
  ## Validate y#
  is_cox <- is.character(family) && family == "cox"#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else if (inherits(family, "family")) {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  } else {#
    stop("'family' must be a GLM family object or the string 'cox'.")#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(is.numeric(offset), length(offset) == n)#
#
  ## Null model fit#
  if (is_cox) {#
    null_res <- null_cox_fit(y, offset = offset, ties = ties)#
  } else {#
    null_res <- null_glm_fit(y, family = family, offset = offset)#
  }#
  BIC_null <- null_res$BIC#
#
  ## Inner function: fit one predictor#
  fit_one_predictor <- function(j) {#
    xj <- X[, j]#
    res <- if (is_cox) {#
      get_cox_fit(x = xj, y = y, offset = offset, standardize = standardize, ties = ties)#
    } else {#
      get_glm_fit(x = xj, y = y, family = family, offset = offset, standardize = standardize)#
    }#
    list(#
      theta   = res$theta,#
      se      = res$se,#
      p_value = res$p_value,#
      logLik  = res$logLik,#
      BIC     = res$BIC#
    )#
  }#
#
  ## Main loop#
  if (parallel) {#
    results <- parallel::mclapply(seq_len(p), fit_one_predictor, mc.cores = parallel::detectCores())#
  } else {#
    results <- lapply(seq_len(p), fit_one_predictor)#
  }#
#
  ## Aggregate results#
  df <- do.call(rbind, lapply(results, as.data.frame))#
#
  ## Row names#
  if (!is.null(colnames(X))) {#
    rownames(df) <- colnames(X)#
  } else {#
    rownames(df) <- paste0("X", seq_len(p))#
  }#
#
  ## Compute Bayes factors and posterior#
  df$delta_BIC <- df$BIC - BIC_null#
  df$delta_BIC <- df$delta_BIC - min(df$delta_BIC)  # stabilization#
  df$bf        <- exp(-0.5 * df$delta_BIC)#
  df$posterior <- df$bf / sum(df$bf)#
#
  df#
}#
#
#' Estimate Intercept Given a Known Offset#
#'#
#' @description#
#' Fit a generalized linear model with a known offset and estimate only the intercept term.#
#' If \code{family = "cox"}, returns \code{0} since Cox models do not have an intercept.#
#'#
#' @param y Response variable.#
#'   \itemize{#
#'     \item For GLMs: a numeric vector or two-column matrix (binomial case).#
#'     \item For Cox models: a \code{n × 2} matrix of \code{(time, status)}.#
#'   }#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or the string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on the linear predictor scale (default \code{0}).#
#'#
#' @return A numeric scalar: the estimated intercept.#
#'#
#' @examples#
#' \dontrun{#
#' # Example: Gaussian GLM#
#' set.seed(1)#
#' X <- rnorm(100)#
#' y <- 2 + 0.5 * X + rnorm(100)#
#' offset <- 0.5 * X#
#' intercept <- update_intercept(y, family = gaussian(), offset = offset)#
#' print(intercept)#
#'#
#' # Example: Binomial GLM#
#' set.seed(2)#
#' X <- rnorm(200)#
#' p <- plogis(-1 + 0.8 * X)#
#' y <- rbinom(200, 1, p)#
#' offset <- 0.8 * X#
#' intercept_binom <- update_intercept(y, family = binomial(), offset = offset)#
#' print(intercept_binom)#
#'#
#' # Example: Cox proportional hazards#
#' library(survival)#
#' set.seed(3)#
#' time <- rexp(150)#
#' status <- rbinom(150, 1, 0.6)#
#' y_cox <- cbind(time, status)#
#' offset <- rep(0, 150)#
#' intercept_cox <- update_intercept(y_cox, family = "cox", offset = offset)#
#' print(intercept_cox)  # Should be 0#
#' }#
#'#
#' @seealso #
#' \code{\link{update_dispersion}} for updating dispersion parameters,#
#' \code{\link{get_glm_family}} for creating GLM family objects,#
#' \code{\link[stats]{glm}} for fitting generalized linear models#
#'#
#' @importFrom stats glm logLik BIC#
#' @export#
update_intercept <- function(#
  y,#
  family,#
  offset = 0#
) {#
  ## Validate offset#
  stopifnot(is.numeric(offset))#
  n <- if (is.matrix(y)) nrow(y) else length(y)#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## Cox model case: no intercept#
  if (is.character(family) && family == "cox") {#
    return(0)#
  }#
#
  ## GLM case#
  if (!inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  data <- data.frame(#
    y   = y,#
    off = offset#
  )#
#
  fit <- stats::glm(#
    formula = y ~ offset(off),#
    data    = data,#
    family  = family#
  )#
#
  intercept_est <- coef(fit)[1]#
#
  return(intercept_est)#
}#
#
#' Update Dispersion Given Known Linear Predictor (Offset)#
#'#
#' @description#
#' Given a known linear predictor (offset), update the dispersion parameter.#
#' For two-parameter GLM families (e.g., Gaussian, Gamma, Inverse Gaussian),#
#' fits a GLM with offset and estimates the dispersion from residuals.#
#' For one-parameter GLM families (e.g., Binomial, Poisson) or Cox models,#
#' returns dispersion = 1.#
#'#
#' @param y Response variable.#
#'   \itemize{#
#'     \item For GLMs: a numeric vector or two-column matrix (binomial).#
#'     \item For Cox model: an \code{n × 2} matrix (time, status).#
#'   }#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{Gamma()}) or the string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known linear predictor (offset).#
#'#
#' @return A numeric scalar: the updated dispersion.#
#'#
#' @examples#
#' \dontrun{#
#' # Gaussian example (true dispersion ~ 2)#
#' set.seed(1)#
#' n <- 100#
#' X <- rnorm(n)#
#' y <- 3 + 1.2 * X + rnorm(n, sd = sqrt(2))  # true sigma^2 = 2#
#' offset <- 3 + 1.2 * X#
#' dispersion_gaussian <- update_dispersion(y, family = gaussian(), offset = offset)#
#' print(dispersion_gaussian)  # Should be close to 2#
#'#
#' # Gamma example (true dispersion ~ 0.5)#
#' set.seed(2)#
#' n <- 200#
#' X <- rnorm(n)#
#' mu <- exp(1.0 + 0.7 * X)#
#' y <- rgamma(n, shape = 2, rate = 2 / mu)  # dispersion = 0.5#
#' offset <- 1.0 + 0.7 * X#
#' dispersion_gamma <- update_dispersion(y, family = Gamma(link = "log"), offset = offset)#
#' print(dispersion_gamma)  # Should be close to 0.5#
#'#
#' # Binomial example (dispersion = 1)#
#' set.seed(3)#
#' n <- 150#
#' X <- rnorm(n)#
#' p <- plogis(0.5 + 0.5 * X)#
#' y <- rbinom(n, 1, p)#
#' offset <- 0.5 + 0.5 * X#
#' dispersion_binomial <- update_dispersion(y, family = binomial(), offset = offset)#
#' print(dispersion_binomial)  # Should be exactly 1#
#'#
#' # Cox example (dispersion = 1)#
#' library(survival)#
#' set.seed(4)#
#' time <- rexp(100)#
#' status <- rbinom(100, 1, 0.7)#
#' y_cox <- cbind(time, status)#
#' dispersion_cox <- update_dispersion(y_cox, family = "cox", offset = rep(0, 100))#
#' print(dispersion_cox)  # Should be exactly 1#
#' }#
#'#
#' @seealso #
#' \code{\link{update_intercept}} for estimating intercepts with offsets,#
#' \code{\link{get_glm_family}} for creating GLM family objects with fixed dispersion,#
#' \code{\link[stats]{summary.glm}} for accessing dispersion estimates#
#'#
#' @importFrom stats glm summary#
#' @export#
update_dispersion <- function(#
  y,#
  family,#
  offset = 0#
) {#
  ## Validate offset#
  stopifnot(is.numeric(offset))#
  n <- if (is.matrix(y)) nrow(y) else length(y)#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## Cox case#
  if (is.character(family) && family == "cox") {#
    return(1)#
  }#
#
  ## GLM case#
  if (!inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  fam_name <- family$family#
#
  ## One-parameter GLM families: binomial, poisson#
  if (fam_name %in% c("binomial", "poisson")) {#
    return(1)#
  }#
#
  ## Two-parameter GLM families: gaussian, Gamma, inverse.gaussian#
  data <- data.frame(#
    y   = y,#
    off = offset#
  )#
#
  fit <- stats::glm(#
    formula = y ~ offset(off),#
    data    = data,#
    family  = family#
  )#
#
  dispersion <- summary(fit)$dispersion#
  return(dispersion)#
}#
#
#' Create a GLM Family with Fixed Dispersion#
#'#
#' @description#
#' Takes an existing family object and optionally fixes the dispersion parameter#
#' (for two-parameter families).#
#'#
#' @param family A \pkg{stats}-style family object (e.g., \code{gaussian()}, \code{binomial()}).#
#' @param dispersion Numeric(1) or \code{NULL}. Fixed dispersion (variance)#
#'   for two-parameter families (Gaussian, Gamma, Inverse-Gaussian).#
#'   Must be > 0 if supplied. Ignored for one-parameter families.#
#' @param validate Logical(1). If \code{TRUE}, performs a quick validity check#
#'   of the constructed family object (default \code{TRUE}).#
#'#
#' @return A modified family object with components:#
#'   \describe{#
#'     \item{variance}{Function \code{mu -> Var(Y)} using fixed dispersion if set.}#
#'     \item{dev.resids}{Function to compute deviance residuals with fixed dispersion.}#
#'     \item{linkfun, linkinv, mu.eta, etc.}{Inherited from the base family.}#
#'     \item{dispersion}{Fixed dispersion value (if two-parameter family).}#
#'     \item{info}{List with \code{family_name} and \code{dispersion}.}#
#'   }#
#'#
#' @examples#
#' # Gaussian with fixed variance#
#' fam1 <- get_glm_family(gaussian("identity"), dispersion = 2)#
#'#
#' # Binomial with logit link#
#' fam2 <- get_glm_family(binomial("logit"))#
#'#
#' # Gamma with fixed dispersion and log link#
#' fam3 <- get_glm_family(Gamma("log"), dispersion = 0.5)#
#'#
#' @seealso #
#' \code{\link[stats]{family}} for standard GLM family objects,#
#' \code{\link{update_dispersion}} for estimating dispersion parameters,#
#' \code{\link[stats]{glm}} for fitting generalized linear models#
#'#
#' @export#
get_glm_family <- function(#
  family,#
  dispersion = NULL,#
  validate   = TRUE#
) {#
  # Ensure family is a family object#
  if (!inherits(family, "family")) {#
    stop("`family` must be a GLM family object.")#
  }#
  # Get family name from the family object#
  family_name <- family$family#
  # Two-parameter check#
  two_param <- family_name %in% c("gaussian", "Gamma", "inverse.gaussian")#
  if (!two_param && !is.null(dispersion)) {#
    warning(#
      "Dispersion ignored for one-parameter family '", family_name, "'."#
    )#
  }#
#
  # Override variance, dev.resids, and aic for two-parameter families#
  if (two_param && !is.null(dispersion)) {#
    stopifnot(is.numeric(dispersion), length(dispersion) == 1, dispersion > 0)#
    if (family_name == "gaussian") {#
      family$variance   <- function(mu) rep(dispersion, length(mu))#
      family$dev.resids <- function(y, mu, wt) wt * ((y - mu)^2) / dispersion#
      family$aic        <- function(y, n, mu, wt, dev)#
        sum(wt) * (log(2 * pi * dispersion) + 1) + dev / dispersion#
    } else if (family_name == "Gamma") {#
      shape <- 1 / dispersion#
      family$variance   <- function(mu) mu^2 / shape#
      family$dev.resids <- function(y, mu, wt)#
        -2 * wt * shape * (log(y/mu) - (y - mu)/mu)#
      family$aic <- function(y, n, mu, wt, dev)#
        -2 * sum(wt * shape * log(shape)) +#
        2 * sum(wt * shape) +#
        2 * sum(wt * lgamma(shape)) +#
        dev / dispersion#
    } else if (family_name == "inverse.gaussian") {#
      family$variance   <- function(mu) mu^3 / dispersion#
      family$dev.resids <- function(y, mu, wt)#
        wt * ((y - mu)^2) / (mu^2 * y * dispersion)#
      family$aic <- function(y, n, mu, wt, dev)#
        sum(wt) * (log(2 * pi * dispersion) - log(y)) + dev / dispersion#
    }#
    family$dispersion <- dispersion#
  }#
#
  ## --- Optional sanity checks of core components ---#
  if (validate) {#
    tryCatch({#
      test_mu <- switch(#
        family_name,#
        binomial         = c(0.2, 0.5, 0.8),#
        inverse.gaussian = c(1, 2, 3),#
        Gamma            = c(1, 2, 3),#
        poisson          = c(1, 2, 3),#
        gaussian         = c(-1, 0, 1)#
      )#
      test_y  <- test_mu#
      test_wt <- rep(1, length(test_mu))#
#
      ## variance and deviance#
      family$variance(test_mu)#
      family$dev.resids(test_y, test_mu, test_wt)#
#
      ## link ↔ inverse-link consistency for unbounded families#
      if (!family_name %in% c("binomial","poisson")) {#
        eta      <- family$linkfun(test_mu)#
        mu_check <- family$linkinv(eta)#
        if (max(abs(test_mu - mu_check)) > 1e-8)#
          warning("linkfun()/linkinv() inconsistency")#
      }#
#
    }, error = function(e) {#
      warning("Family validation failed: ", e$message)#
    })#
  }#
#
  # Annotate and return#
  family$info <- list(#
    family_name = family_name,#
    dispersion  = dispersion#
  )#
  family#
}#
#
#' Fit a univariate Cox proportional hazards model with optional standardization#
#'#
#' @description#
#' Fit a univariate Cox proportional hazards model for a single predictor#
#' \code{x} on survival data given by \code{y}, with an optional offset.#
#' Predictor \code{x} may be centered and scaled (glmnet-style) if#
#' \code{standardize = TRUE}.  Ties in event times are handled by the#
#' specified method (default "efron" as in \pkg{survival}::\code{coxph}).#
#'#
#' @param x            Numeric vector of length \code{n}.  Predictor values.#
#' @param y            Numeric.  Either#
#'   \itemize{#
#'     \item A numeric vector of event times (if all observations are events),#
#'     \item OR a numeric \code{n x 2} matrix where the first column is#
#'       follow-up time and the second column is the event indicator#
#'       (1 = event, 0 = censored).#
#'   }#
#' @param offset       Numeric scalar or vector of length \code{n}.  Known offset#
#'                     on the linear predictor (default \code{0}).#
#' @param standardize  Logical(1).  If \code{TRUE}, center and scale#
#'                     \code{x} by its L2-norm (default \code{TRUE}).#
#' @param ties         Character(1).  Method for handling ties: one of#
#'                     \code{"efron"}, \code{"breslow"}, \code{"exact"},#
#'                     or \code{"user"}.  Default \code{"efron"}.#
#'#
#' @return A named list with components:#
#' \describe{#
#'   \item{theta}{Estimated log-hazard ratio (slope) on the original \code{x} scale.}#
#'   \item{se}{Standard error of \code{theta}.}#
#'   \item{p_value}{Wald test p-value for \code{theta = 0}.}#
#'   \item{logLik}{Partial log-likelihood of the fitted Cox model.}#
#'   \item{BIC}{Bayesian Information Criterion:#
#'               \(-2\ell + \log(n)\times df\).}#
#'   \item{fit}{The full \code{coxph} object returned by#
#'            \pkg{survival}::\code{coxph()}.}#
#' }#
#'#
#' @examples#
#' \dontrun{#
#' library(survival)#
#' n      <- 200#
#' x      <- rnorm(n)#
#' tt     <- rexp(n, rate = exp(0.5 * x))#
#' status <- rbinom(n, 1, 0.7)#
#' y_mat  <- cbind(tt, status)#
#'#
#' res1 <- get_cox_fit(x, y_mat,#
#'                    offset      = 0,#
#'                    standardize = TRUE,#
#'                    ties        = "efron")#
#'#
#' res2 <- get_cox_fit(x, cbind(tt, status),#
#'                    offset      = 0,#
#'                    standardize = FALSE,#
#'                    ties        = "breslow")#
#' }#
#'#
#' @seealso #
#' \code{\link[survival]{coxph}} for fitting Cox proportional hazards models,#
#' \code{\link[survival]{Surv}} for creating survival objects,#
#' \code{\link{get_ser_fit}} for fitting univariate models across multiple predictors#
#'#
#' @import survival#
#' @export#
get_cox_fit <- function(#
  x,#
  y,#
  ties        = c("efron", "breslow", "exact", "user"),#
  offset      = 0,#
  standardize = TRUE#
) {#
  ## --- validate inputs ---#
  stopifnot(is.numeric(x))#
  n <- length(x)#
  stopifnot(is.numeric(y) || is.matrix(y))#
  if (is.matrix(y)) {#
    stopifnot(nrow(y) == n, ncol(y) == 2)#
    time   <- y[, 1]#
    status <- y[, 2]#
  } else {#
    stop("`y` must be an n×2 matrix of (time, status).")#
  }#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
  stopifnot(is.logical(standardize), length(standardize) == 1L)#
  ties <- match.arg(ties)#
#
  ## --- standardize predictor if requested ---#
  if (standardize) {#
    x_mean <- mean(x)#
    x_norm <- sqrt(sum((x - x_mean)^2))#
    if (x_norm <= 0) {#
      x_std  <- x#
      x_norm <- 1#
      x_mean <- 0#
    } else {#
      x_std <- (x - x_mean) / x_norm#
    }#
  } else {#
    x_mean <- 0; x_norm <- 1; x_std <- x#
  }#
#
  ## --- fit Cox model ---#
  df <- data.frame(#
    time   = time,#
    status = status,#
    x      = x_std,#
    off    = offset#
  )#
  fit <- survival::coxph(#
    survival::Surv(time, status) ~ x + offset(off),#
    data = df,#
    ties = ties#
  )#
#
  ## --- extract estimates ---#
  coefs     <- summary(fit)$coefficients#
  theta_std <- coefs["x", "coef"]#
  se_std    <- coefs["x", "se(coef)"]#
  p_val     <- coefs["x", "Pr(>|z|)"]#
#
  ## --- back-transform to original scale ---#
  theta <- theta_std / x_norm#
  se    <- se_std    / x_norm#
#
  ## --- compute logLik and BIC ---#
  ll_obj     <- stats::logLik(fit)#
  logLik_val <- as.numeric(ll_obj)#
  df_fit     <- attr(ll_obj, "df")#
  bic_val    <- -2 * logLik_val + log(n) * df_fit#
#
  list(#
    theta   = theta,#
    se      = se,#
    p_value = p_val,#
    logLik  = logLik_val,#
    BIC     = bic_val,#
    fit     = fit#
  )#
}#
#
#' Fit a Univariate GLM with Optional Standardization#
#'#
#' @description#
#' Fit a univariate generalized linear model for a single predictor \code{x}#
#' and response \code{y}, using a specified GLM family object and an optional#
#' known offset. Optionally perform glmnet-style standardization of \code{x} and#
#' center \code{y} when using a Gaussian family.#
#'#
#' @param x Numeric vector of length \code{n}. Predictor values.#
#' @param y Numeric vector of length \code{n}. Response values. For binomial,#
#'   should be 0/1 or a two-column matrix.#
#' @param family A GLM family object, as returned by \code{get_glm_family()} or#
#'   one of \code{gaussian()}, \code{binomial()}, etc.#
#' @param offset Numeric scalar or vector of length \code{n}. Known offset on#
#'   the linear predictor scale. Default is \code{0}.#
#' @param standardize Logical; if \code{TRUE}, center and scale \code{x} by its#
#'   L2-norm. Default is \code{TRUE}.#
#'#
#' @return A named list with elements:#
#' \describe{#
#'   \item{theta}{Estimated slope coefficient on the original \code{x} scale.}#
#'   \item{se}{Standard error of \code{theta} on the original scale.}#
#'   \item{p_value}{Wald test p-value for testing \code{theta = 0}.}#
#'   \item{logLik}{Model log-likelihood (numeric scalar).}#
#'   \item{BIC}{Bayesian Information Criterion (numeric scalar).}#
#'   \item{fit}{The full \code{glm} object returned by \code{stats::glm()}.}#
#' }#
#'#
#' @examples#
#' \dontrun{#
#' famG <- get_glm_family(gaussian(), dispersion = 2)#
#' x    <- rnorm(100); y <- 1 + 2*x + rnorm(100, sd = sqrt(2))#
#' res  <- get_glm_fit(x, y, family = famG, offset = 0, standardize = TRUE)#
#'#
#' famB <- get_glm_family(binomial())#
#' x    <- rnorm(200); y <- rbinom(200, 1, plogis(-1 + 0.5*x))#
#' res  <- get_glm_fit(x, y, family = famB, offset = -1, standardize = TRUE)#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}} for fitting across multiple predictors,#
#' \code{\link{get_glm_family}} for creating GLM family objects,#
#' \code{\link[stats]{glm}} for fitting generalized linear models#
#'#
#' @export#
get_glm_fit <- function(#
  x,#
  y,#
  family,#
  offset = 0,#
  standardize = TRUE#
) {#
  stopifnot(is.numeric(x), is.numeric(y))#
  n <- length(y)#
  stopifnot(length(x) == n)#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  # Standardize predictor if requested#
  if (standardize) {#
    x_mean <- mean(x)#
    x_norm <- sqrt(sum((x - x_mean)^2))#
    if (x_norm == 0) {#
      x_std  <- x#
      x_norm <- 1#
      x_mean <- 0#
    } else {#
      x_std <- (x - x_mean) / x_norm#
    }#
  } else {#
    x_mean <- 0#
    x_norm <- 1#
    x_std  <- x#
  }#
#
  # Center response for Gaussian family#
  fam_name <- family$family#
  if (fam_name == "gaussian") {#
    y_mean <- mean(y)#
    y_use  <- y - y_mean#
  } else {#
    y_mean <- 0#
    y_use  <- y#
  }#
#
  # Fit GLM with intercept and known offset#
  dat <- data.frame(y = y_use, x = x_std, off = offset)#
  fit <- stats::glm(y ~ x + offset(off), data = dat, family = family)#
#
  # Extract standardized coefficients#
  sm <- summary(fit)$coefficients#
  theta_std <- sm[2, 1]  # second row = slope for x#
  se_std    <- sm[2, 2]#
  p_val     <- sm[2, 4]#
#
  # Back-transform to original scale#
  theta <- theta_std / x_norm#
  se    <- se_std    / x_norm#
#
  # Model fit statistics#
  ll  <- as.numeric(stats::logLik(fit))#
  bic <- as.numeric(stats::BIC(fit))#
#
  list(#
    theta   = theta,#
    se      = se,#
    p_value = p_val,#
    logLik  = ll,#
    BIC     = bic,#
    fit     = fit#
  )#
}#
#
#' Fit the Null (offset-only) GLM and Return Model Statistics#
#'#
#' @description#
#' Fit a generalized linear model with no covariates (only a known offset) and#
#' return its log-likelihood and Bayesian information criterion (BIC).#
#'#
#' @param y Numeric vector of length \code{n}. The response variable.#
#' @param family A \pkg{stats}-style GLM family object (e.g., from#
#'   \code{get_glm_family()} or \code{gaussian()}, \code{binomial()}, etc.).#
#' @param offset Numeric scalar or vector of length \code{n}. Known offset on#
#'   the linear predictor scale. Default is \code{0}.#
#'#
#' @return A named list with components:#
#' \describe{#
#'   \item{logLik}{Numeric scalar; the fitted model log-likelihood.}#
#'   \item{BIC}{Numeric scalar; the model's Bayesian information criterion.}#
#'   \item{fit}{The full \code{glm} object returned by \code{stats::glm()}.}#
#' }#
#'#
#' @examples#
#' \dontrun{#
#' fam <- get_glm_family(gaussian(), dispersion = 1)#
#' y   <- rnorm(100)#
#' res <- null_glm_fit(y, family = fam)#
#' res$logLik#
#' res$BIC#
#' }#
#'#
#' @seealso #
#' \code{\link{null_cox_fit}} for null Cox model fitting,#
#' \code{\link{get_ser_fit}} for comparing models across predictors,#
#' \code{\link[stats]{BIC}} for Bayesian Information Criterion#
#'#
#' @keywords internal#
#' @export#
null_glm_fit <- function(#
  y,#
  family,#
  offset = 0#
) {#
  stopifnot(is.numeric(y))#
  n <- length(y)#
  if (length(offset) == 1L) {#
    offset <- rep(offset, n)#
  }#
  stopifnot(length(offset) == n)#
#
  data <- data.frame(#
    y   = y,#
    off = offset#
  )#
#
  fit_null <- stats::glm(#
    formula = y ~ offset(off) - 1,#
    data    = data,#
    family  = family#
  )#
#
  logLik_val <- as.numeric(stats::logLik(fit_null))#
  bic_val    <- as.numeric(stats::BIC(fit_null))#
#
  list(#
    logLik = logLik_val,#
    BIC    = bic_val,#
    fit    = fit_null#
  )#
}#
#
#' Fit the Null (Offset-Only) Cox Proportional Hazards Model#
#'#
#' @description#
#' Fit a Cox proportional hazards model with only a known offset (no covariates)#
#' and return its partial log-likelihood and Bayesian Information Criterion (BIC).#
#'#
#' @param y A \code{Surv} object or an n × 2 matrix (time, status), where n is the number of observations.#
#' @param offset Numeric scalar or vector of length n. Known offset on#
#'   the linear predictor (log-hazard) scale. Default is \code{0}.#
#' @param ties Character(1). Method for handling ties in the Cox model;#
#'   one of \code{"efron"}, \code{"breslow"}, \code{"exact"}, \code{"user"} (default \code{"efron"}).#
#'#
#' @return A named list with components:#
#' \describe{#
#'   \item{logLik}{Numeric scalar; the partial log-likelihood of the fitted null Cox model.}#
#'   \item{BIC}{Numeric scalar; Bayesian Information Criterion, computed as \eqn{-2 \times logLik}.}#
#'   \item{fit}{The fitted \code{coxph} object.}#
#' }#
#'#
#' @examples#
#' \dontrun{#
#' library(survival)#
#' time   <- rexp(100)#
#' status <- rbinom(100, 1, 0.7)#
#' y      <- Surv(time, status)#
#' res    <- null_cox_fit(y, offset = rep(0, 100), ties = "efron")#
#' res$logLik#
#' res$BIC#
#' }#
#'#
#' @seealso #
#' \code{\link{null_glm_fit}} for null GLM fitting,#
#' \code{\link[survival]{coxph}} for Cox proportional hazards models,#
#' \code{\link{get_ser_fit}} for comparing models across predictors#
#'#
#' @keywords internal#
#' @export#
null_cox_fit <- function(y, offset = 0, ties = "efron") {#
  ## Validate y#
  if (!inherits(y, "Surv")) {#
    if (!is.matrix(y) || ncol(y) != 2) {#
      stop("If `y` is not a Surv object, it must be an n × 2 matrix (time, status).")#
    }#
    y <- survival::Surv(y[, 1], y[, 2])#
  }#
  n <- NROW(y)#
#
  ## Validate offset#
  if (length(offset) == 1L) {#
    offset <- rep(offset, n)#
  }#
  stopifnot(is.numeric(offset), length(offset) == n)#
#
  ## Fit model#
  df <- data.frame(off = offset)#
  fit_null <- survival::coxph(#
    formula = y ~ offset(off) - 1,#
    data    = df,#
    ties    = ties#
  )#
#
  ## Extract second log-likelihood (model with offset)#
  logLik_val <- fit_null$loglik#
  bic_val    <- -2 * logLik_val#
#
  list(#
    logLik = logLik_val,#
    BIC    = bic_val,#
    fit    = fit_null#
  )#
}
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 20#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 100#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 20#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 5#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
fit
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 20#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
fit
round(fit$posterior,2)
fit$theta
rowSums(fit$theta)
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 20#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 2#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 3#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1.5, 2), rep(0, p - 2))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
#
  ## --- Return results ---#
  list(#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 50#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 50#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE
#' Fit LASER Model (L-Effect Sparse Regression)#
#'#
#' @description#
#' Block coordinate ascent algorithm to fit LASER (sum of L sparse effects) models,#
#' supporting GLM families and Cox regression.#
#'#
#' @param X Numeric matrix or vector (n × p). Predictor matrix or vector.#
#' @param y Response variable (vector for GLM; matrix (time, status) for Cox).#
#' @param L Integer. Number of effects. Default \code{min(10, p)}.#
#' @param family A GLM family object (e.g., \code{gaussian()}, \code{binomial()}) or string \code{"cox"}.#
#' @param offset Numeric scalar or vector. Known offset on linear predictor (default \code{0}).#
#' @param standardize Logical. Whether to standardize predictors (default \code{TRUE}).#
#' @param ties Character. Tie handling method for Cox models (default \code{"efron"}).#
#' @param parallel Logical. Whether to use parallel computation (default \code{TRUE}).#
#' @param max_iter Integer. Maximum number of block coordinate ascent iterations (default \code{10}).#
#'#
#' @return A list containing:#
#' \describe{#
#'   \item{intercept}{Estimated intercept.}#
#'   \item{dispersion}{Estimated dispersion.}#
#'   \item{theta}{Updated theta matrix (p × L).}#
#'   \item{posterior}{Posterior weights matrix (p × L).}#
#' }#
#'#
#' @seealso #
#' \code{\link{get_ser_fit}}, #
#' \code{\link{get_glm_family}}, #
#' \code{\link{update_intercept}}, #
#' \code{\link{update_dispersion}}#
#'#
#' @import survival#
#' @export#
get_laser_fit <- function(#
  X,#
  y,#
  L = NULL,#
  family,#
  offset = 0,#
  standardize = TRUE,#
  ties = c("efron", "breslow", "exact", "user"),#
  parallel = TRUE,#
  max_iter = 10#
) {#
  start.time <- Sys.time()#
  ## --- Validate and prepare inputs ---#
  if (is.data.frame(X)) X <- as.matrix(X)#
  if (is.null(dim(X))) X <- matrix(X, ncol = 1)#
  stopifnot(is.numeric(X))#
#
  n <- nrow(X)#
  p <- ncol(X)#
  if (is.null(L)) L <- min(10, p)#
#
  ## Validate family#
  is_cox <- is.character(family) && family == "cox"#
  if (!is_cox && !inherits(family, "family")) {#
    stop("`family` must be a GLM family object or 'cox'.")#
  }#
#
  ## Validate response#
  if (is_cox) {#
    ties <- match.arg(ties)#
    stopifnot(is.matrix(y), nrow(y) == n, ncol(y) == 2)#
  } else {#
    stopifnot((is.vector(y) && length(y) == n) || (is.matrix(y) && nrow(y) == n))#
  }#
#
  ## Validate offset#
  if (length(offset) == 1L) offset <- rep(offset, n)#
  stopifnot(length(offset) == n)#
#
  ## --- Initialize parameters ---#
  intercept  <- 0#
  dispersion <- 1#
  theta      <- matrix(0, nrow = p, ncol = L)#
  posterior  <- matrix(0, nrow = p, ncol = L)#
#
  ## --- Main optimization loop ---#
  for (iter in seq_len(max_iter)) {#
#
    ## Refresh GLM family with current dispersion if not Cox#
    reg_family <- if (!is_cox) {#
      get_glm_family(family, dispersion = dispersion)#
    } else {#
      "cox"#
    }#
#
    ## --- Update theta and posterior for each effect ---#
    for (l in seq_len(L)) {#
      offset_excluding_l <- intercept + X %*% rowSums(theta[, -l, drop = FALSE])#
#
      ser_fit <- get_ser_fit(#
        X           = X,#
        y           = y,#
        family      = reg_family,#
        offset      = offset_excluding_l,#
        standardize = standardize,#
        ties        = ties,#
        parallel    = parallel#
      )#
#
      theta[, l]     <- ser_fit$theta * ser_fit$posterior#
      posterior[, l] <- ser_fit$posterior#
    }#
#
    ## --- Update intercept ---#
    offset_without_intercept <- X %*% rowSums(theta)#
    intercept <- update_intercept(#
      y      = y,#
      family = family,#
      offset = offset_without_intercept#
    )#
#
    ## --- Update dispersion ---#
    full_offset <- intercept + offset_without_intercept#
    dispersion <- update_dispersion(#
      y      = y,#
      family = family,#
      offset = full_offset#
    )#
  }#
  end.time <- Sys.time()#
  ## --- Return results ---#
  list(#
    elaspse = (end.time - start.time)/60,#
    intercept = intercept,#
    dispersion = dispersion,#
    theta = theta,#
    posterior = posterior#
  )#
}#
# Gaussian example#
set.seed(1)#
n <- 200#
p <- 50#
X <- matrix(rnorm(n * p), nrow = n)#
beta <- c(rep(1, 5), rep(0, p - 5))#
y <- 2 + X %*% beta + rnorm(n)#
standardize = TRUE#
L <- 10#
family <- gaussian()#
ties = c("efron", "breslow", "exact", "user")#
max_iter <- 10#
parallel = TRUE#
#
fit <- get_laser_fit(#
  X = X,#
  y = y,#
  L = L,#
  family = family,#
  max_iter = max_iter,#
  parallel = parallel#
)#
#
fit$elaspse
setwd("~/Documents/glmcs")
setwd("~/Documents/glmcs")
# Install required packages if needed
library(usethis)
devtools::load_all()
devtools::test()
devtools::load_all()
devtools::test()
devtools::load_all()
devtools::test()
setwd("~/Documents/glmcs")#
source("R/model.R")#
source("R/utils.R")#
library(Rcpp)#
sourceCpp("src/core.cpp")#
sourceCpp("src/model.cpp")
# SETUP: Create controlled test data with multicollinearity#
  set.seed(12)#
  n <- 200                                     # Number of observations#
  p <- 10                                      # Number of predictors#
  # Generate base independent random variables#
  Z <- matrix(rnorm(n * p), n, p)#
  # Create highly correlated pairs#
  X <- matrix(0, n, p)#
  X[, 1] <- Z[, 1]#
  X[, 2] <- 0.95 * Z[, 1] + 0.05 * Z[, 2]      # X1, X2 correlated at ~0.95#
  X[, 3] <- Z[, 3]#
  X[, 4] <- 0.95 * Z[, 3] + 0.05 * Z[, 4]      # X3, X4 correlated at ~0.95#
  # Generate remaining predictors independently#
  X[, 5:p] <- Z[, 5:p]#
  # Generate binary response - only X1 and X4 have true effects#
  true_pair1 <- c(1, 2)                       # First correlated pair#
  true_pair2 <- c(3, 4)                       # Second correlated pair#
  linear_pred <- X[, 1] + X[, 4] - 1          # Intercept = -1#
  prob <- 1/(1 + exp(-linear_pred))           # Inverse logit function#
  y <- rbinom(n, 1, prob)                     # Binary outcome#
  # EXECUTE: Run laser algorithm#
  output <- laser(X, y, family = binomial())#
  # VERIFY: Test output structure and correctness#
  # 1. Basic structure checks#
  expect_s3_class(output, "laser")#
  expect_true(all(c("cs", "fit") %in% names(output)))#
  # 2. Confidence sets validation#
  expect_type(output$cs, "list")#
  expect_true(all(c("sets", "coverage") %in% names(output$cs)))#
  expect_type(output$cs$sets, "list")#
  # 3. Variable selection correctness under multicollinearity#
  # Check if we find at least one variable from each correlated pair#
  found_pair1 <- FALSE#
  found_pair2 <- FALSE#
  for (i in seq_along(output$cs$sets)) {#
    set_vars <- output$cs$sets[[i]]#
    if (any(true_pair1 %in% set_vars)) {#
      found_pair1 <- TRUE#
    }#
    if (any(true_pair2 %in% set_vars)) {#
      found_pair2 <- TRUE#
    }#
  }
# SETUP: Create controlled test data with multicollinearity#
  set.seed(12)#
  n <- 200                                     # Number of observations#
  p <- 10                                      # Number of predictors#
  # Generate base independent random variables#
  Z <- matrix(rnorm(n * p), n, p)#
  # Create highly correlated pairs#
  X <- matrix(0, n, p)#
  X[, 1] <- Z[, 1]#
  X[, 2] <- 0.95 * Z[, 1] + 0.05 * Z[, 2]      # X1, X2 correlated at ~0.95#
  X[, 3] <- Z[, 3]#
  X[, 4] <- 0.95 * Z[, 3] + 0.05 * Z[, 4]      # X3, X4 correlated at ~0.95#
  # Generate remaining predictors independently#
  X[, 5:p] <- Z[, 5:p]#
  # Generate binary response - only X1 and X4 have true effects#
  true_pair1 <- c(1, 2)                       # First correlated pair#
  true_pair2 <- c(3, 4)                       # Second correlated pair#
  linear_pred <- X[, 1] + X[, 4] - 1          # Intercept = -1#
  prob <- 1/(1 + exp(-linear_pred))           # Inverse logit function#
  y <- rbinom(n, 1, prob)                     # Binary outcome#
  # EXECUTE: Run laser algorithm#
  output <- laser(X, y, family = binomial())
# VERIFY: Test output structure and correctness
# 1. Basic structure checks
expect_s3_class(output, "laser")
expect_true(all(c("cs", "fit") %in% names(output)))
# 2. Confidence sets validation
expect_type(output$cs, "list")
expect_true(all(c("sets", "coverage") %in% names(output$cs)))
expect_type(output$cs$sets, "list")
output$cs$sets
# SETUP: Create controlled test data with multicollinearity#
  set.seed(12)#
  n <- 500                                     # Number of observations#
  p <- 10                                      # Number of predictors#
  # Generate base independent random variables#
  Z <- matrix(rnorm(n * p), n, p)#
  # Create highly correlated pairs#
  X <- matrix(0, n, p)#
  X[, 1] <- Z[, 1]#
  X[, 2] <- 0.95 * Z[, 1] + 0.05 * Z[, 2]      # X1, X2 correlated at ~0.95#
  X[, 3] <- Z[, 3]#
  X[, 4] <- 0.95 * Z[, 3] + 0.05 * Z[, 4]      # X3, X4 correlated at ~0.95#
  # Generate remaining predictors independently#
  X[, 5:p] <- Z[, 5:p]#
  # Generate binary response - only X1 and X4 have true effects#
  true_pair1 <- c(1, 2)                       # First correlated pair#
  true_pair2 <- c(3, 4)                       # Second correlated pair#
  linear_pred <- X[, 1] + X[, 4] - 1          # Intercept = -1#
  prob <- 1/(1 + exp(-linear_pred))           # Inverse logit function#
  y <- rbinom(n, 1, prob)                     # Binary outcome#
  # EXECUTE: Run laser algorithm#
  output <- laser(X, y, family = binomial())#
  # VERIFY: Test output structure and correctness#
  # 1. Basic structure checks#
  expect_s3_class(output, "laser")#
  expect_true(all(c("cs", "fit") %in% names(output)))#
  # 2. Confidence sets validation#
  expect_type(output$cs, "list")#
  expect_true(all(c("sets", "coverage") %in% names(output$cs)))#
  expect_type(output$cs$sets, "list")
# 3. Variable selection correctness under multicollinearity
# Check if we find at least one variable from each correlated pair
found_pair1 <- FALSE
found_pair2 <- FALSE
for (i in seq_along(output$cs$sets)) {#
    set_vars <- output$cs$sets[[i]]#
    if (any(true_pair1 %in% set_vars)) {#
      found_pair1 <- TRUE#
    }#
    if (any(true_pair2 %in% set_vars)) {#
      found_pair2 <- TRUE#
    }#
  }
expect_true(found_pair1, "No confidence set contains any variable from the first correlated pair")
expect_true(found_pair2, "No confidence set contains any variable from the second correlated pair")
# 4. Check coefficient magnitudes - should reflect uncertainty due to correlation
var1_coef <- abs(output$fit$theta_hat[1])
var2_coef <- abs(output$fit$theta_hat[2])
var3_coef <- abs(output$fit$theta_hat[3])
var4_coef <- abs(output$fit$theta_hat[4])
# Either one variable from each pair should have substantial coefficient#
  # or both variables in the pair should have similar coefficients#
  expect_true(#
    (var1_coef > 0.3 || var2_coef > 0.3) && #
    abs(var1_coef - var2_coef) < 0.5,#
    "Coefficients for first pair don't reflect the correlation structure"#
  )#
  expect_true(#
    (var3_coef > 0.3 || var4_coef > 0.3) && #
    abs(var3_coef - var4_coef) < 0.5,#
    "Coefficients for second pair don't reflect the correlation structure"#
  )
# SETUP: Create controlled test data with multicollinearity#
  set.seed(1)#
  n <- 500                                     # Number of observations#
  p <- 10                                      # Number of predictors#
  # Generate base independent random variables#
  Z <- matrix(rnorm(n * p), n, p)#
  # Create highly correlated pairs#
  X <- matrix(0, n, p)#
  X[, 1] <- Z[, 1]#
  X[, 2] <- 0.95 * Z[, 1] + 0.05 * Z[, 2]      # X1, X2 correlated at ~0.95#
  X[, 3] <- Z[, 3]#
  X[, 4] <- 0.95 * Z[, 3] + 0.05 * Z[, 4]      # X3, X4 correlated at ~0.95#
  # Generate remaining predictors independently#
  X[, 5:p] <- Z[, 5:p]#
  # Generate survival data - only X1 and X4 have true effects#
  true_pair1 <- c(1, 2)                       # First correlated pair#
  true_pair2 <- c(3, 4)                       # Second correlated pair#
  # Create survival data with effects from X1 and X4#
  linear_pred <- X[, 1] + X[, 4]#
  baseline_hazard <- rexp(n, 0.1)             # Baseline hazard#
  hazard <- baseline_hazard * exp(linear_pred) # Proportional hazards#
  time <- rexp(n, hazard)                     # Event times#
  # Add censoring#
  censor_time <- rexp(n, 0.05)                # Censoring times#
  observed_time <- pmin(time, censor_time)    # Observed time is minimum#
  status <- as.numeric(time <= censor_time)   # Event indicator#
  # Create survival response#
  y <- cbind(observed_time, status)#
  # EXECUTE: Run laser algorithm#
  output <- laser(X, y, family = "cox")#
  # VERIFY: Test output structure and correctness#
  # 1. Basic structure checks#
  expect_s3_class(output, "laser")#
  expect_true(all(c("cs", "fit") %in% names(output)))#
  # 2. Confidence sets validation#
  expect_type(output$cs, "list")#
  expect_true(all(c("sets", "coverage") %in% names(output$cs)))#
  expect_type(output$cs$sets, "list")#
  # 3. Variable selection correctness under multicollinearity#
  # Check if we find at least one variable from each correlated pair#
  found_pair1 <- FALSE#
  found_pair2 <- FALSE#
  for (i in seq_along(output$cs$sets)) {#
    set_vars <- output$cs$sets[[i]]#
    if (any(true_pair1 %in% set_vars)) {#
      found_pair1 <- TRUE#
    }#
    if (any(true_pair2 %in% set_vars)) {#
      found_pair2 <- TRUE#
    }#
  }#
  expect_true(found_pair1, "No confidence set contains any variable from the first correlated pair")#
  expect_true(found_pair2, "No confidence set contains any variable from the second correlated pair")#
  # 4. Check for uncertainty in the estimated coefficients#
  # At least one variable from each pair should have non-negligible coefficient#
  max_coef_pair1 <- max(abs(output$fit$theta_hat[true_pair1]))#
  max_coef_pair2 <- max(abs(output$fit$theta_hat[true_pair2]))#
  expect_true(max_coef_pair1 > 0.3, "No effect detected for the first correlated pair")#
  expect_true(max_coef_pair2 > 0.3, "No effect detected for the second correlated pair")#
  # 5. Model fit quality#
  expect_true(output$fit$final_loglike > -Inf)
output$fit
output$cs
max_coef_pair1
rm(list = c(#
  "get_cox_fit", "get_cs", "get_family", "get_glm_fit",#
  "get_included", "get_laser_fit", "get_loglike",#
  "get_purity", "get_ser_fit", "kl_divergence", "laser",#
  "null_cox_fit", "null_glm_fit", "update_dispersion",#
  "update_intercept"#
))#
devtools::load_all()
devtools::test()
usethis::use_readme_md()
usethis::use_gpl3_license()  # Or another license of your choice
usethis::use_git_ignore(c("*.o", "*.so", "*.dll", ".Rproj.user", ".Rhistory", ".RData", ".DS_Store"))
install.packages("glmcs")
install.packages("remotes")
remotes::install_github("yizenglistat/glmcs")
library(glmcs)
?laser
remove.packages('glmcs')
devtools::load_all()
devtools::load_all()
